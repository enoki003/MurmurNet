#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
MurmurNet ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
åˆ†æ•£SLMã‚·ã‚¹ãƒ†ãƒ ã®ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
å¯¾è©±çš„ã«è³ªå•å¿œç­”ã‚’è¡Œã†ãŸã‚ã®ã‚³ãƒ³ã‚½ãƒ¼ãƒ«UI

ä½œè€…: Yuhi Sonoki
"""
# C:\Users\åœ’æœ¨å„ªé™½\AppData\Roaming\kiwix-desktop\wikipedia_en_top_nopic_2025-03.zim
import sys
import os
import argparse
import asyncio
import signal
from pathlib import Path
import logging
import atexit

# murmurnetãƒ‘ã‚¹ã‚’è¿½åŠ 
current_dir = Path(__file__).resolve().parent
project_root = current_dir.parent
sys.path.append(str(project_root))  # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 

# ã“ã“ã§ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from murmurnet.distributed_slm import DistributedSLM

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªSLMã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³ç”¨ï¼‰
_global_slm = None

# ãƒ­ã‚°è¨­å®š
parser = argparse.ArgumentParser(description="MurmurNet Console App - åˆ†æ•£SLMã‚·ã‚¹ãƒ†ãƒ ")
parser.add_argument('--debug', action='store_true', help='ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¡¨ç¤º')
parser.add_argument('--log', action='store_true', help='ãƒ­ã‚°ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜')
parser.add_argument('--iterations', '--iter', '-iter', type=int, default=1, 
                    help='åå¾©å›æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1ï¼‰ â€»--iter, -iter ã§ã‚‚æŒ‡å®šå¯èƒ½')
parser.add_argument('--agents', type=int, default=2, help='ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2ï¼‰')
parser.add_argument('--no-summary', action='store_true', help='è¦ç´„æ©Ÿèƒ½ã‚’ç„¡åŠ¹åŒ–')
parser.add_argument('--parallel', action='store_true', help='ä¸¦åˆ—å‡¦ç†ã‚’æœ‰åŠ¹åŒ–')
parser.add_argument('--model-type', choices=['llama', 'huggingface'], required=True,
                    help='ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ï¼ˆå¿…é ˆï¼‰: llama=Gemmaãƒ¢ãƒ‡ãƒ«, huggingface=HuggingFaceãƒ¢ãƒ‡ãƒ«')
parser.add_argument('--model-name', '--huggingface-model', type=str, default=None,
                    help='ãƒ¢ãƒ‡ãƒ«åï¼ˆmodel-type=huggingfaceæ™‚ã¯å¿…é ˆã€150Mã¸ã®è‡ªå‹•ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç¦æ­¢ï¼‰')
parser.add_argument('--model-path', type=str, default=None,
                    help='ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆmodel-type=llamaæ™‚ã¯å¿…é ˆï¼‰')

# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚ªãƒ—ã‚·ãƒ§ãƒ³
parser.add_argument('--no-local-files', action='store_true', 
                    help='ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ¢ãƒ¼ãƒ‰ã‚’ç„¡åŠ¹åŒ–ï¼ˆHuggingFaceã¸ã®HTTPã‚¢ã‚¯ã‚»ã‚¹ã‚’è¨±å¯ï¼‰')
parser.add_argument('--cache-folder', type=str, 
                    default=r"C:\Users\admin\Desktop\èª²é¡Œç ”ç©¶\ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹\MurmurNet\cache\models",
                    help='ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹')

# RAGãƒ¢ãƒ¼ãƒ‰ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ 
parser.add_argument('--rag-mode', choices=['dummy', 'zim'], default='dummy', 
                    help='RAGãƒ¢ãƒ¼ãƒ‰ï¼ˆdummy: ãƒ€ãƒŸãƒ¼ãƒ¢ãƒ¼ãƒ‰ã€zim: ZIMãƒ•ã‚¡ã‚¤ãƒ«ä½¿ç”¨ï¼‰')
parser.add_argument('--zim-path', type=str, 
                    default=r"C:\Users\admin\Desktop\èª²é¡Œç ”ç©¶\KNOWAGE_DATABASE\wikipedia_en_top_nopic_2025-03.zim",
                    help='ZIMãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆRAGãƒ¢ãƒ¼ãƒ‰ãŒzimã®å ´åˆã«ä½¿ç”¨ï¼‰')
# ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆ¥ãƒ¢ãƒ‡ãƒ«è¨­å®šã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ 
parser.add_argument('--internal-model', type=str, default=None,
                    help='å†…éƒ¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç”¨ã®ãƒ¢ãƒ‡ãƒ«åï¼ˆHuggingFaceå½¢å¼ï¼‰')
parser.add_argument('--output-model', type=str, default=None,
                    help='å‡ºåŠ›ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç”¨ã®ãƒ¢ãƒ‡ãƒ«åï¼ˆHuggingFaceå½¢å¼ï¼‰')
parser.add_argument('--summary-model', type=str, default=None,
                    help='è¦ç´„ã‚¨ãƒ³ã‚¸ãƒ³ç”¨ã®ãƒ¢ãƒ‡ãƒ«åï¼ˆHuggingFaceå½¢å¼ï¼‰')

# ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆ¥ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã¨ãƒ‘ã‚¹æŒ‡å®šã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ 
parser.add_argument('--internal-model-type', choices=['llama', 'huggingface'], default=None,
                    help='å†…éƒ¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç”¨ã®ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—')
parser.add_argument('--output-model-type', choices=['llama', 'huggingface'], default=None,
                    help='å‡ºåŠ›ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç”¨ã®ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—')
parser.add_argument('--summary-model-type', choices=['llama', 'huggingface'], default=None,
                    help='è¦ç´„ã‚¨ãƒ³ã‚¸ãƒ³ç”¨ã®ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—')

parser.add_argument('--internal-model-path', type=str, default=None,
                    help='å†…éƒ¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç”¨ã®ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆGGUFå½¢å¼ï¼‰')
parser.add_argument('--output-model-path', type=str, default=None,
                    help='å‡ºåŠ›ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç”¨ã®ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆGGUFå½¢å¼ï¼‰')
parser.add_argument('--summary-model-path', type=str, default=None,
                    help='è¦ç´„ã‚¨ãƒ³ã‚¸ãƒ³ç”¨ã®ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆGGUFå½¢å¼ï¼‰')

# ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆ¥ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
parser.add_argument('--internal-temp', type=float, default=None,
                    help='å†…éƒ¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.7ï¼‰')
parser.add_argument('--output-temp', type=float, default=None,
                    help='å‡ºåŠ›ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.7ï¼‰')
parser.add_argument('--summary-temp', type=float, default=None,
                    help='è¦ç´„ã‚¨ãƒ³ã‚¸ãƒ³ã®æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.6ï¼‰')

parser.add_argument('--internal-tokens', type=int, default=None,
                    help='å†…éƒ¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 200ï¼‰')
parser.add_argument('--output-tokens', type=int, default=None,
                    help='å‡ºåŠ›ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 250ï¼‰')
parser.add_argument('--summary-tokens', type=int, default=None,
                    help='è¦ç´„ã‚¨ãƒ³ã‚¸ãƒ³ã®æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 150ï¼‰')

# å…¨ä½“çš„ãªç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆç©ºãƒ¬ã‚¹å¯¾ç­–ï¼‰
parser.add_argument('--max-new-tokens', type=int, default=128,
                    help='æœ€å¤§æ–°è¦ãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 128ã€ç©ºãƒ¬ã‚¹å¯¾ç­–ï¼‰')

# ä¸¦åˆ—å‡¦ç†ã®å®‰å…¨æ€§ã«é–¢ã™ã‚‹ã‚ªãƒ—ã‚·ãƒ§ãƒ³
parser.add_argument('--safe-parallel', action='store_true', 
                    help='å®‰å…¨ãªä¸¦åˆ—å‡¦ç†ãƒ¢ãƒ¼ãƒ‰ï¼ˆGGMLã‚¨ãƒ©ãƒ¼å›é¿ã®ãŸã‚ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ­ãƒƒã‚¯ã‚’ä½¿ç”¨ï¼‰')
parser.add_argument('--max-workers', type=int, default=0, 
                    help='ä¸¦åˆ—å‡¦ç†æ™‚ã®æœ€å¤§ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ï¼ˆ0: è‡ªå‹•æ±ºå®šï¼‰')

# Slotã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚ªãƒ—ã‚·ãƒ§ãƒ³
parser.add_argument('--slots', action='store_true', 
                    help='Slotã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãƒ¢ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹åŒ–ï¼ˆå¤šè§’çš„è­°è«–ã‚·ã‚¹ãƒ†ãƒ ï¼‰')
parser.add_argument('--parallel-only', action='store_true',
                    help='ä¸¦åˆ—å®Ÿè¡Œã®ã¿ï¼ˆè­°è«–ãƒ»ç›¸äº’å‚ç…§ã‚’ç„¡åŠ¹åŒ–ï¼‰')
# Sloté–¢é€£ã®è©³ç´°è¨­å®š
parser.add_argument('--slot-temperature', type=float, default=0.7,
                    help='Slotã®æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.7ï¼‰')
parser.add_argument('--slot-max-length', type=int, default=300,
                    help='Slotã®æœ€å¤§å‡ºåŠ›é•·ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 300ï¼‰')
parser.add_argument('--slot-similarity-threshold', type=float, default=0.7,
                    help='Sloté–“ã®é¡ä¼¼åº¦é–¾å€¤ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.7ï¼‰')
args, _ = parser.parse_known_args()

log_level = logging.DEBUG if args.debug else logging.INFO
logging.basicConfig(
    filename='console_app.log' if args.log else None,
    level=log_level,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
# ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«ã‚‚ãƒ­ã‚°å‡ºåŠ›ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›æ™‚ã®ã¿è¿½åŠ ï¼‰
if args.log:
    console = logging.StreamHandler()
    console.setLevel(log_level)
    console.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
    logging.getLogger('').addHandler(console)

# libzimãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
try:
    from libzim.reader import Archive
    HAS_LIBZIM = True
    print("libzimãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨å¯èƒ½ã§ã™")
except ImportError:
    HAS_LIBZIM = False
    print("è­¦å‘Š: libzimãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
    print("ZIMãƒ¢ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ã€ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„:")
    print("pip install libzim")
    if args.rag_mode == "zim":
        print("ZIMãƒ¢ãƒ¼ãƒ‰ãŒæŒ‡å®šã•ã‚Œã¾ã—ãŸãŒã€libzimãŒãªã„ãŸã‚dummyãƒ¢ãƒ¼ãƒ‰ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™")

def print_debug(slm):
    """ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰æ™‚ã®è©³ç´°æƒ…å ±è¡¨ç¤º"""
    print("\n[DEBUG] é»’æ¿ã®å†…å®¹:")
    # ç°¡ç•¥åŒ–ã•ã‚ŒãŸãƒ“ãƒ¥ãƒ¼ã‚’ä½¿ç”¨
    debug_view = slm.blackboard.get_debug_view()
    for k, v in debug_view.items():
        print(f"  {k}: {v}")
    
    print("\n[DEBUG] RAGçµæœ:")
    print(f"  {slm.blackboard.read('rag')}")
    
    # Slotãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯Slotæƒ…å ±ã‚’è¡¨ç¤º
    if slm.use_slots and hasattr(slm.blackboard, 'get_slot_debug_view'):
        print("\n[DEBUG] Slotæƒ…å ±:")
        slot_debug = slm.blackboard.get_slot_debug_view()
        slot_info = slot_debug.get('slot_info', {})
        print(f"  ç™»éŒ²Slotæ•°: {len(slot_info.get('registered_slots', []))}")
        print(f"  ç·ã‚¨ãƒ³ãƒˆãƒªæ•°: {slot_info.get('total_entries', 0)}")
        
        for slot_name, summary in slot_info.get('slot_summaries', {}).items():
            latest_text = summary.get('latest_text', '')
            if latest_text:
                preview = latest_text[:50] + "..." if len(latest_text) > 50 else latest_text
                print(f"    {slot_name}: {preview}")
    else:
        # è¦ç´„çµæœã‚’è¡¨ç¤ºï¼ˆå¾“æ¥ãƒ¢ãƒ¼ãƒ‰ï¼‰
        if slm.use_summary:
            print("\n[DEBUG] è¦ç´„çµæœ:")
            for i in range(slm.iterations):
                summary = slm.blackboard.read(f'summary_{i}')
                if summary:
                    print(f"  åå¾©{i+1}ã®è¦ç´„: {summary[:100]}...")
        
        print("\n[DEBUG] ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå‡ºåŠ›:")
        for i in range(slm.num_agents):
            output = slm.blackboard.read(f'agent_{i}_output')
            if output:
                # é•·ã„å‡ºåŠ›ã¯çœç•¥è¡¨ç¤º
                if len(output) > 100:
                    output = output[:100] + "..."
                print(f"  ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ{i+1}: {output}")
    print()

async def safe_shutdown(slm):
    """å®‰å…¨ãªã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³å‡¦ç†"""
    global _global_slm
    
    try:
        # 1. SLMã®ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³
        if hasattr(slm, 'shutdown'):
            print("SLMã‚·ã‚¹ãƒ†ãƒ ã‚’ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³ä¸­...")
            await slm.shutdown()
        
        # 2. InputReceptionã®å¼·åˆ¶çµ‚äº†
        try:
            from murmurnet.modules.input_reception import InputReception
            print("InputReceptionã‚’çµ‚äº†ä¸­...")
            InputReception.force_exit_all()
        except Exception as e:
            print(f"InputReceptionçµ‚äº†ã‚¨ãƒ©ãƒ¼: {e}")
          # 3. DistributedSLMã®ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãï¼‰
        if _global_slm:
            try:
                print("DistributedSLMã‚’ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³ä¸­...")
                # 15ç§’ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã§ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³
                await asyncio.wait_for(_global_slm.shutdown(), timeout=15.0)
                print("DistributedSLMã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³å®Œäº†")
            except asyncio.TimeoutError:
                print("DistributedSLMã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ - å¼·åˆ¶çµ‚äº†ã—ã¾ã™")
            except Exception as e:
                print(f"DistributedSLMã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³ã‚¨ãƒ©ãƒ¼: {e}")
        
        # 4. æœ€çµ‚ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        _global_slm = None
        
        print("å…¨ã¦ã®ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ")
        
    except Exception as e:
        print(f"safe_shutdownå†…ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
        raise

def _build_agent_models_config(args):
    """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆ¥ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’æ§‹ç¯‰ï¼ˆæ··åœ¨ä½¿ç”¨å¯¾å¿œï¼‰"""
    agent_models = {}
    
    # å†…éƒ¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¨­å®š
    internal_config = {}
    if args.internal_model or args.internal_model_path:
        # ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã‚’æ±ºå®šï¼ˆæ˜ç¤ºçš„æŒ‡å®š or æ¨è«–ï¼‰
        if args.internal_model_type:
            internal_config['model_type'] = args.internal_model_type
        elif args.internal_model_path:
            internal_config['model_type'] = 'llama'
        elif args.internal_model:
            internal_config['model_type'] = 'huggingface'
        else:
            internal_config['model_type'] = args.model_type
        
        # ãƒ¢ãƒ‡ãƒ«æŒ‡å®š
        if internal_config['model_type'] == 'huggingface':
            internal_config['model_name'] = args.internal_model
        elif internal_config['model_type'] == 'llama':
            internal_config['model_path'] = args.internal_model_path
    
    if args.internal_temp is not None:
        internal_config['temperature'] = args.internal_temp
    if args.internal_tokens is not None:
        internal_config['max_tokens'] = args.internal_tokens
    
    if internal_config:
        agent_models['internal_agents'] = internal_config
    
    # å‡ºåŠ›ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¨­å®š
    output_config = {}
    if args.output_model or args.output_model_path:
        # ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã‚’æ±ºå®š
        if args.output_model_type:
            output_config['model_type'] = args.output_model_type
        elif args.output_model_path:
            output_config['model_type'] = 'llama'
        elif args.output_model:
            output_config['model_type'] = 'huggingface'
        else:
            output_config['model_type'] = args.model_type
        
        # ãƒ¢ãƒ‡ãƒ«æŒ‡å®š
        if output_config['model_type'] == 'huggingface':
            output_config['model_name'] = args.output_model
        elif output_config['model_type'] == 'llama':
            output_config['model_path'] = args.output_model_path
    
    if args.output_temp is not None:
        output_config['temperature'] = args.output_temp
    if args.output_tokens is not None:
        output_config['max_tokens'] = args.output_tokens
    
    if output_config:
        agent_models['output_agent'] = output_config
    
    # è¦ç´„ã‚¨ãƒ³ã‚¸ãƒ³è¨­å®š
    summary_config = {}
    if args.summary_model or args.summary_model_path:
        # ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã‚’æ±ºå®š
        if args.summary_model_type:
            summary_config['model_type'] = args.summary_model_type
        elif args.summary_model_path:
            summary_config['model_type'] = 'llama'
        elif args.summary_model:
            summary_config['model_type'] = 'huggingface'
        else:
            summary_config['model_type'] = args.model_type
        
        # ãƒ¢ãƒ‡ãƒ«æŒ‡å®š
        if summary_config['model_type'] == 'huggingface':
            summary_config['model_name'] = args.summary_model
        elif summary_config['model_type'] == 'llama':
            summary_config['model_path'] = args.summary_model_path
    
    if args.summary_temp is not None:
        summary_config['temperature'] = args.summary_temp
    if args.summary_tokens is not None:
        summary_config['max_tokens'] = args.summary_tokens
    
    if summary_config:
        agent_models['summary_engine'] = summary_config
    
    return agent_models

async def chat_loop(args):
    """ä¼šè©±ãƒ«ãƒ¼ãƒ—ã®ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    # è¨­å®š
    config = {
        # "model_path": r"C:\Users\åœ’æœ¨å„ªé™½\OneDrive\ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—\models\gemma-3-1b-it-q4_0.gguf",
        # "chat_template": r"C:\Users\åœ’æœ¨å„ªé™½\OneDrive\ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—\models\gemma3_template.txt",
        "model_path": r"C:\Users\admin\Desktop\èª²é¡Œç ”ç©¶\models\gemma-3-1b-it-q4_0.gguf",
        "chat_template": r"C:\Users\admin\Desktop\èª²é¡Œç ”ç©¶\models\gemma3_template.txt",

        "num_agents": args.agents,
        "iterations": args.iterations,
        "use_summary": not args.no_summary,
        "use_parallel": args.parallel,
        "debug": args.debug,
        
        # ãƒ¢ãƒ‡ãƒ«è¨­å®šã®ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰
        "model_type": args.model_type if args.model_type else "llama",  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯llama
        "huggingface_model_name": args.model_name,  # --model-name ã«å¤‰æ›´
        "model_path": args.model_path,  # ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹è¿½åŠ 
        "device": "cpu",  # CPUã‚’ä½¿ç”¨
        "torch_dtype": "auto",
        
        # ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆç©ºãƒ¬ã‚¹å¯¾ç­–ï¼‰
        "max_new_tokens": args.max_new_tokens,  # CLIå¼•æ•°ã‹ã‚‰å–å¾—
        
        # åŸºæœ¬è¨­å®š
        "local_files_only": not args.no_local_files,  # --no-local-filesãƒ•ãƒ©ã‚°ã«åŸºã¥ã
        "cache_folder": args.cache_folder,  # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‹ã‚‰å–å¾—
        
        # RAGè¨­å®šï¼ˆåŸºæœ¬è¨­å®šï¼‰
        "rag_mode": "local",  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆRAGãƒ¢ãƒ¼ãƒ‰
        "rag_score_threshold": 0.5,
        "rag_top_k": 3,
        "embedding_model": "all-MiniLM-L6-v2",
        "embedding_cache_folder": args.cache_folder,  # åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ç”¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚©ãƒ«ãƒ€
        
        # ä¸¦åˆ—å‡¦ç†è¨­å®š
        "use_global_lock": True,  # GGMLã‚¨ãƒ©ãƒ¼å›é¿ã®ãŸã‚ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ­ãƒƒã‚¯
        
        # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆ¥ãƒ¢ãƒ‡ãƒ«è¨­å®š
        "agent_models": _build_agent_models_config(args)
    }

    # CLIå¼•æ•°ã‚’æœ€å¾Œã«ä¸Šæ›¸ãï¼ˆCLIå¼•æ•°ã®å„ªå…ˆé †ä½ã‚’ç¢ºä¿ï¼‰
    config.update(vars(args))

    # åŸºæœ¬è¨­å®šå®Œäº†
    print(f"è¨­å®šå®Œäº†: ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—={config['model_type']}")
    if config['model_type'] == 'huggingface':
        print(f"HuggingFaceãƒ¢ãƒ‡ãƒ«: {config['huggingface_model_name']}")
    elif config['model_type'] == 'llama':
        print(f"Llamaãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹: {config.get('model_path', 'ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ')}")
    
    # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®šã®è¡¨ç¤º
    print(f"ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ¢ãƒ¼ãƒ‰: {config['local_files_only']}")
    print(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚©ãƒ«ãƒ€: {config['cache_folder']}")
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚©ãƒ«ãƒ€ã®å­˜åœ¨ç¢ºèª
    import os
    if not os.path.exists(config['cache_folder']):
        print(f"è­¦å‘Š: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚©ãƒ«ãƒ€ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {config['cache_folder']}")
        try:
            os.makedirs(config['cache_folder'], exist_ok=True)
            print(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆã—ã¾ã—ãŸ: {config['cache_folder']}")
        except Exception as e:
            print(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚©ãƒ«ãƒ€ã®ä½œæˆã«å¤±æ•—: {e}")
            
    # HuggingFaceãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ã‚«ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèª
    if config['model_type'] == 'huggingface' and config['local_files_only']:
        model_cache_path = os.path.join(config['cache_folder'], f"models--{config['huggingface_model_name'].replace('/', '--')}")
        if os.path.exists(model_cache_path):
            print(f"ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ: {model_cache_path}")
        else:
            print(f"è­¦å‘Š: ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_cache_path}")
            print("ãƒ¢ãƒ‡ãƒ«ã‚’äº‹å‰ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‹ã€--no-local-files ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„")
        
    # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆ¥ãƒ¢ãƒ‡ãƒ«è¨­å®šã®è¡¨ç¤º
    if config.get('agent_models'):
        print("\n=== ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆ¥ãƒ¢ãƒ‡ãƒ«è¨­å®š ===")
        agent_models = config['agent_models']
        
        if 'internal_agents' in agent_models:
            internal = agent_models['internal_agents']
            model_info = internal.get('model_name') or internal.get('model_path', 'å…±é€šè¨­å®š')
            model_type = internal.get('model_type', 'unknown')
            print(f"å†…éƒ¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ: {model_info} ({model_type}) "
                  f"(æ¸©åº¦: {internal.get('temperature', 'å…±é€šè¨­å®š')}, "
                  f"ãƒˆãƒ¼ã‚¯ãƒ³: {internal.get('max_tokens', 'å…±é€šè¨­å®š')})")
        
        if 'output_agent' in agent_models:
            output = agent_models['output_agent']
            model_info = output.get('model_name') or output.get('model_path', 'å…±é€šè¨­å®š')
            model_type = output.get('model_type', 'unknown')
            print(f"å‡ºåŠ›ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ: {model_info} ({model_type}) "
                  f"(æ¸©åº¦: {output.get('temperature', 'å…±é€šè¨­å®š')}, "
                  f"ãƒˆãƒ¼ã‚¯ãƒ³: {output.get('max_tokens', 'å…±é€šè¨­å®š')})")
        
        if 'summary_engine' in agent_models:
            summary = agent_models['summary_engine']
            model_info = summary.get('model_name') or summary.get('model_path', 'å…±é€šè¨­å®š')
            model_type = summary.get('model_type', 'unknown')
            print(f"è¦ç´„ã‚¨ãƒ³ã‚¸ãƒ³: {model_info} ({model_type}) "
                  f"(æ¸©åº¦: {summary.get('temperature', 'å…±é€šè¨­å®š')}, "
                  f"ãƒˆãƒ¼ã‚¯ãƒ³: {summary.get('max_tokens', 'å…±é€šè¨­å®š')})")
        print("=" * 35)
    else:
        print("\nå…¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§å…±é€šãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨")
    
    # Slotã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­å®š
    if args.slots:
        print(f"\n=== Slotã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­å®š ===")
        config['use_slots'] = True
        config['slot_temperature'] = args.slot_temperature
        config['slot_max_output_length'] = args.slot_max_length
        config['slot_similarity_threshold'] = args.slot_similarity_threshold
        
        # å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰è¨­å®šï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯å”èª¿ï¼‰
        if args.parallel_only:
            config['use_collaboration'] = False
            print("ğŸ“‹ ä¸¦åˆ—å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰: SlotãŒç‹¬ç«‹å®Ÿè¡Œ")
        else:
            config['use_collaboration'] = True
            print("ğŸ¤ å”èª¿ãƒ¢ãƒ¼ãƒ‰: SlotãŒè­°è«–ãƒ»ç›¸äº’å‚ç…§")
        
        # Slotãƒ¢ãƒ¼ãƒ‰æ™‚ã¯å¾“æ¥ã®Agentã¯ç„¡åŠ¹åŒ–
        config['iterations'] = 1  # Slotã¯å˜ä¸€å®Ÿè¡Œ
        config['use_summary'] = False  # å¾“æ¥ã®è¦ç´„ã¯ç„¡åŠ¹
        
        print(f"Slotæ¸©åº¦: {config['slot_temperature']}")
        print(f"Slotæœ€å¤§é•·: {config['slot_max_output_length']}")
        print(f"é¡ä¼¼åº¦é–¾å€¤: {config['slot_similarity_threshold']}")
        print("=" * 30)
    else:
        config['use_slots'] = False
        config['use_collaboration'] = False
        
    # SLMã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ
    global _global_slm
    slm = DistributedSLM(config)
    _global_slm = slm  # ã‚°ãƒ­ãƒ¼ãƒãƒ«å‚ç…§ã‚’è¨­å®š
    
    mode_info = "Slotã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£" if args.slots else f"{args.agents}ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ, {args.iterations}åå¾©"
    if args.slots and args.parallel_only:
        mode_info += " (ä¸¦åˆ—å®Ÿè¡Œ)"
    elif args.slots:
        mode_info += " (å”èª¿ãƒ¢ãƒ¼ãƒ‰)"
    print(f"MurmurNet Console ({mode_info})")
    print("çµ‚äº†ã™ã‚‹ã«ã¯ 'quit' ã¾ãŸã¯ 'exit' ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    
    if args.parallel and not args.slots:
        print("[è¨­å®š] ä¸¦åˆ—å‡¦ç†: æœ‰åŠ¹")
    if not args.no_summary and not args.slots:
        print("[è¨­å®š] è¦ç´„æ©Ÿèƒ½: æœ‰åŠ¹")
    if args.slots:
        collaboration_mode = "ä¸¦åˆ—å®Ÿè¡Œ" if args.parallel_only else "å”èª¿ãƒ¢ãƒ¼ãƒ‰"
        print(f"[è¨­å®š] Slotã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£: æœ‰åŠ¹ ({collaboration_mode})")
    print(f"[è¨­å®š] RAGãƒ¢ãƒ¼ãƒ‰: {config['rag_mode']}")
    print(f"[è¨­å®š] ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ¢ãƒ¼ãƒ‰: {config['local_files_only']}")
    if args.debug:
        print(f"[DEBUG] ä½¿ç”¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚©ãƒ«ãƒ€: {config['cache_folder']}")
        print(f"[DEBUG] åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚©ãƒ«ãƒ€: {config['embedding_cache_folder']}")
        if config['model_type'] == 'huggingface':
            model_cache_path = os.path.join(config['cache_folder'], f"models--{config['huggingface_model_name'].replace('/', '--')}")
            print(f"[DEBUG] æœŸå¾…ã•ã‚Œã‚‹ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‘ã‚¹: {model_cache_path}")
            print(f"[DEBUG] ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥å­˜åœ¨: {os.path.exists(model_cache_path)}")
            # åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèª
            embed_cache_path = os.path.join(config['embedding_cache_folder'], f"models--sentence-transformers--{config['embedding_model']}")
            print(f"[DEBUG] æœŸå¾…ã•ã‚Œã‚‹åŸ‹ã‚è¾¼ã¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‘ã‚¹: {embed_cache_path}")
            print(f"[DEBUG] åŸ‹ã‚è¾¼ã¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥å­˜åœ¨: {os.path.exists(embed_cache_path)}")
    
    history = []
    while True:
        try:            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
            user_input = input("\nã‚ãªãŸ> ")
            if user_input.lower() in ["quit", "exit", "çµ‚äº†"]:
                print("ã‚·ã‚¹ãƒ†ãƒ ã‚’çµ‚äº†ã—ã¦ã„ã¾ã™...")
                # é©åˆ‡ãªã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³å‡¦ç†
                try:
                    print("å®Œå…¨ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³ã‚’é–‹å§‹...")
                    await safe_shutdown(slm)
                    print("å…¨ã¦ã®ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ")
                except Exception as e:
                    print(f"ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³ã‚¨ãƒ©ãƒ¼: {e}")
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå¼·åˆ¶çµ‚äº†
                    try:
                        cleanup_system()
                    except:
                        pass
                finally:
                    # ç¢ºå®Ÿã«ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†
                    print("ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³å®Œäº†")
                    import os
                    os._exit(0)
            
            # ç©ºå…¥åŠ›ã¯ã‚¹ã‚­ãƒƒãƒ—
            if not user_input.strip():
                continue
            
            # å±¥æ­´ã«è¿½åŠ 
            history.append({"role": "user", "content": user_input})
            
            # ç”Ÿæˆé–‹å§‹
            print("AI> ", end="", flush=True)
            
            start_time = asyncio.get_event_loop().time()
            response = await slm.generate(user_input)
            elapsed = asyncio.get_event_loop().time() - start_time
            
            # å¿œç­”è¡¨ç¤º
            print(f"{response}")
            
            # ãƒ‡ãƒãƒƒã‚°æƒ…å ±è¡¨ç¤º
            if args.debug:
                print_debug(slm)
                print(f"[DEBUG] å®Ÿè¡Œæ™‚é–“: {elapsed:.2f}ç§’")            # å±¥æ­´ã«è¿½åŠ 
            history.append({"role": "assistant", "content": response})
            
        except KeyboardInterrupt:
            print("\nä¸­æ–­ã•ã‚Œã¾ã—ãŸã€‚ã‚·ã‚¹ãƒ†ãƒ ã‚’ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³ã—ã¾ã™...")
            try:
                await safe_shutdown(slm)
            except:
                cleanup_system()
            finally:
                import os
                os._exit(0)
        except Exception as e:
            print(f"\nã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            if args.debug:
                import traceback
                traceback.print_exc()

def cleanup_system():
    """ã‚·ã‚¹ãƒ†ãƒ ã®ç·Šæ€¥ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    global _global_slm
    if _global_slm:
        try:
            print("ã‚·ã‚¹ãƒ†ãƒ ã‚’ç·Šæ€¥ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³ä¸­...")
            # éåŒæœŸé–¢æ•°ã‚’åŒæœŸã§å®Ÿè¡Œ
            try:
                loop = asyncio.get_event_loop()
                if loop.is_running():
                    # æ—¢ã«ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ãŒå‹•ã„ã¦ã„ã‚‹å ´åˆ
                    asyncio.create_task(_global_slm.shutdown())
                else:
                    loop.run_until_complete(_global_slm.shutdown())
            except RuntimeError:
                # ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ãŒãªã„å ´åˆã¯æ–°ã—ãä½œæˆ
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                loop.run_until_complete(_global_slm.shutdown())
                loop.close()
        except Exception as e:
            print(f"ç·Šæ€¥ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³ã‚¨ãƒ©ãƒ¼: {e}")
        finally:
            _global_slm = None

def signal_handler(signum, frame):
    """ã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒ©"""
    print(f"\nã‚·ã‚°ãƒŠãƒ« {signum} ã‚’å—ä¿¡ã—ã¾ã—ãŸã€‚ã‚·ã‚¹ãƒ†ãƒ ã‚’çµ‚äº†ã—ã¾ã™...")
    cleanup_system()
    sys.exit(0)

# ã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒ©ã¨atexitç™»éŒ²
signal.signal(signal.SIGINT, signal_handler)
if hasattr(signal, 'SIGTERM'):
    signal.signal(signal.SIGTERM, signal_handler)
atexit.register(cleanup_system)

def validate_args(args):
    """CLIå¼•æ•°ã®æ¤œè¨¼ï¼ˆ150Mãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç¦æ­¢ï¼‰"""
    errors = []
    
    # model-typeå¿…é ˆãƒã‚§ãƒƒã‚¯
    if not args.model_type:
        errors.append("--model-type ã¯å¿…é ˆã§ã™ã€‚'llama' ã¾ãŸã¯ 'huggingface' ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")
    
    # model-typeåˆ¥ã®å¿…é ˆå¼•æ•°ãƒã‚§ãƒƒã‚¯
    if args.model_type == 'llama':
        if not args.model_path:
            errors.append("--model-type=llama ã®å ´åˆã€--model-path ã¯å¿…é ˆã§ã™ã€‚")
        elif not os.path.exists(args.model_path):
            errors.append(f"æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {args.model_path}")
    
    elif args.model_type == 'huggingface':
        if not args.model_name:
            errors.append("--model-type=huggingface ã®å ´åˆã€--model-name ã¯å¿…é ˆã§ã™ã€‚")
            errors.append("150Mãƒ¢ãƒ‡ãƒ«ã¸ã®è‡ªå‹•ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¯ç¦æ­¢ã•ã‚Œã¦ã„ã¾ã™ã€‚")
        
        # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ¢ãƒ¼ãƒ‰æ™‚ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        if not args.no_local_files:
            model_cache_path = os.path.join(args.cache_folder, f"models--{args.model_name.replace('/', '--')}")
            if not os.path.exists(model_cache_path):
                errors.append(f"ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ¢ãƒ¼ãƒ‰ã§ã™ãŒã€ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_cache_path}")
                errors.append("ãƒ¢ãƒ‡ãƒ«ã‚’äº‹å‰ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‹ã€--no-local-files ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚")
    
    # iterationsç¯„å›²ãƒã‚§ãƒƒã‚¯
    if args.iterations < 1 or args.iterations > 10:
        errors.append("--iterations ã¯1-10ã®ç¯„å›²ã§æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")
    
    # agentsç¯„å›²ãƒã‚§ãƒƒã‚¯
    if args.agents < 1 or args.agents > 20:
        errors.append("--agents ã¯1-20ã®ç¯„å›²ã§æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")
    
    # ã‚¨ãƒ©ãƒ¼ãŒã‚ã‚‹å ´åˆã¯è¡¨ç¤ºã—ã¦çµ‚äº†
    if errors:
        print("=== å¼•æ•°ã‚¨ãƒ©ãƒ¼ ===")
        for error in errors:
            print(f"ã‚¨ãƒ©ãƒ¼: {error}")
        print("\nä½¿ç”¨ä¾‹:")
        print("  Llamaãƒ¢ãƒ‡ãƒ«ä½¿ç”¨:")
        print("    python console_app.py --model-type llama --model-path ./models/model.gguf")
        print("  HuggingFaceãƒ¢ãƒ‡ãƒ«ä½¿ç”¨:")
        print("    python console_app.py --model-type huggingface --model-name rinna/japanese-gpt2-medium")
        print("  åå¾©å›æ•°æŒ‡å®š:")
        print("    python console_app.py --model-type llama --model-path ./model.gguf --iterations 3")
        sys.exit(1)

if __name__ == "__main__":
    # å¼•æ•°ã‚’ãƒ‘ãƒ¼ã‚¹
    args = parser.parse_args()
    
    # å¼•æ•°ã‚’æ¤œè¨¼
    validate_args(args)
    
    print("=== MurmurNet Console App ===")
    print(f"ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—: {args.model_type}")
    if args.model_type == 'llama':
        print(f"ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹: {args.model_path}")
    elif args.model_type == 'huggingface':
        print(f"ãƒ¢ãƒ‡ãƒ«å: {args.model_name}")
    print(f"åå¾©å›æ•°: {args.iterations}")
    print(f"ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ•°: {args.agents}")
    print("=" * 30)
    
    # ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—å®Ÿè¡Œ
    asyncio.run(chat_loop(args))
