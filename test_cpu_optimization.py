#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
MurmurNet CPUæœ€é©åŒ–ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
CPUæœ€é©åŒ–ã®åŠ¹æœã‚’æ¸¬å®šã—ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã‚’è©•ä¾¡ã™ã‚‹

ä½œè€…: Yuhi Sonoki
"""

import os
import sys
import time
import yaml
import asyncio
import logging
from typing import Dict, Any, List

# ãƒ‘ã‚¹ã®è¨­å®š
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from MurmurNet.distributed_slm import DistributedSLM
from MurmurNet.modules.performance import PerformanceMonitor

class CPUOptimizationTester:
    """CPUæœ€é©åŒ–åŠ¹æœã®ãƒ†ã‚¹ãƒˆãƒ»è©•ä¾¡ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, config_path: str = "config.yaml"):
        """
        ãƒ†ã‚¹ã‚¿ãƒ¼ã®åˆæœŸåŒ–
        
        å¼•æ•°:
            config_path: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        """
        self.config_path = config_path
        self.config = self.load_config()
        
        # ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger('CPUOptimizationTester')
        
        # ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
        self.test_cases = [
            "æ—¥æœ¬ã®äººå£ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚",
            "AIã¨æ©Ÿæ¢°å­¦ç¿’ã®é•ã„ã¯ä½•ã§ã™ã‹ï¼Ÿ",
            "ç’°å¢ƒå•é¡Œã®è§£æ±ºç­–ã«ã¤ã„ã¦è­°è«–ã—ã¦ãã ã•ã„ã€‚",
            "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã‚’å­¦ã¶åŠ¹æœçš„ãªæ–¹æ³•ã¯ï¼Ÿ",
            "å¥åº·çš„ãªç”Ÿæ´»ç¿’æ…£ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚"
        ]
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–
        self.performance = PerformanceMonitor(enabled=True, memory_tracking=True)
        
    def load_config(self) -> Dict[str, Any]:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except Exception as e:
            self.logger.error(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return {}
    
    async def run_single_test(self, question: str, config: Dict[str, Any]) -> Dict[str, Any]:
        """
        å˜ä¸€ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã®å®Ÿè¡Œ
        
        å¼•æ•°:
            question: ãƒ†ã‚¹ãƒˆè³ªå•
            config: è¨­å®šè¾æ›¸
            
        æˆ»ã‚Šå€¤:
            ãƒ†ã‚¹ãƒˆçµæœ
        """
        self.logger.info(f"ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ: {question[:50]}...")
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®šé–‹å§‹
        start_time = time.time()
        initial_memory = self.performance.get_memory_usage()
        initial_cpu = self.performance.get_cpu_usage()
        
        try:
            # MurmurNetã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®ä½œæˆ
            murmur_net = DistributedSLM(config)
            
            # å¿œç­”ç”Ÿæˆ
            response = await murmur_net.generate(question)
            
            # æ¸¬å®šçµ‚äº†
            end_time = time.time()
            final_memory = self.performance.get_memory_usage()
            final_cpu = self.performance.get_cpu_usage()
            
            # çµæœã®æ•´ç†
            result = {
                'question': question,
                'response': response,
                'execution_time': end_time - start_time,
                'response_length': len(response),
                'tokens_per_second': len(response.split()) / (end_time - start_time) if end_time > start_time else 0,
                'memory_usage': {
                    'initial_rss': initial_memory.get('rss', 0),
                    'final_rss': final_memory.get('rss', 0),
                    'memory_increase': final_memory.get('rss', 0) - initial_memory.get('rss', 0)
                },
                'cpu_usage': {
                    'initial_cpu': initial_cpu.get('system_cpu_percent', 0),
                    'final_cpu': final_cpu.get('system_cpu_percent', 0)
                },
                'config_used': {
                    'num_agents': config.get('num_agents', 2),
                    'iterations': config.get('iterations', 1),
                    'use_parallel': config.get('use_parallel', False),
                    'n_threads': config.get('n_threads', 4)
                }
            }
            
            return result
            
        except Exception as e:
            self.logger.error(f"ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            return {
                'question': question,
                'error': str(e),
                'execution_time': time.time() - start_time
            }
    
    async def run_performance_comparison(self) -> Dict[str, Any]:
        """
        ä¸¦åˆ—å‡¦ç†ã¨é€æ¬¡å‡¦ç†ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ
        
        æˆ»ã‚Šå€¤:
            æ¯”è¼ƒçµæœ
        """
        self.logger.info("=== ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒãƒ†ã‚¹ãƒˆé–‹å§‹ ===")
        
        results = {
            'sequential': [],
            'parallel': [],
            'comparison': {}
        }
        
        # é€æ¬¡å‡¦ç†ãƒ†ã‚¹ãƒˆ
        self.logger.info("1. é€æ¬¡å‡¦ç†ãƒ†ã‚¹ãƒˆ")
        sequential_config = self.config.copy()
        sequential_config['use_parallel'] = False
        sequential_config['num_agents'] = 3
        sequential_config['iterations'] = 2
        
        for question in self.test_cases:
            result = await self.run_single_test(question, sequential_config)
            results['sequential'].append(result)
            await asyncio.sleep(1)  # ã‚·ã‚¹ãƒ†ãƒ å®‰å®šåŒ–ã®ãŸã‚ã®å¾…æ©Ÿ
        
        # ä¸¦åˆ—å‡¦ç†ãƒ†ã‚¹ãƒˆ
        self.logger.info("2. ä¸¦åˆ—å‡¦ç†ãƒ†ã‚¹ãƒˆ")
        parallel_config = self.config.copy()
        parallel_config['use_parallel'] = True
        parallel_config['num_agents'] = 3
        parallel_config['iterations'] = 2
        
        for question in self.test_cases:
            result = await self.run_single_test(question, parallel_config)
            results['parallel'].append(result)
            await asyncio.sleep(1)  # ã‚·ã‚¹ãƒ†ãƒ å®‰å®šåŒ–ã®ãŸã‚ã®å¾…æ©Ÿ
        
        # æ¯”è¼ƒåˆ†æ
        results['comparison'] = self.analyze_results(
            results['sequential'], 
            results['parallel']
        )
        
        return results
    
    def analyze_results(self, sequential_results: List[Dict], parallel_results: List[Dict]) -> Dict[str, Any]:
        """
        ãƒ†ã‚¹ãƒˆçµæœã®åˆ†æ
        
        å¼•æ•°:
            sequential_results: é€æ¬¡å‡¦ç†çµæœ
            parallel_results: ä¸¦åˆ—å‡¦ç†çµæœ
            
        æˆ»ã‚Šå€¤:
            åˆ†æçµæœ
        """
        def calculate_average(results: List[Dict], key: str) -> float:
            valid_results = [r for r in results if key in r and 'error' not in r]
            if not valid_results:
                return 0.0
            return sum(r[key] for r in valid_results) / len(valid_results)
        
        # å¹³å‡å®Ÿè¡Œæ™‚é–“
        seq_avg_time = calculate_average(sequential_results, 'execution_time')
        par_avg_time = calculate_average(parallel_results, 'execution_time')
        
        # å¹³å‡ãƒˆãƒ¼ã‚¯ãƒ³/ç§’
        seq_avg_tps = calculate_average(sequential_results, 'tokens_per_second')
        par_avg_tps = calculate_average(parallel_results, 'tokens_per_second')
        
        # ã‚¹ãƒ”ãƒ¼ãƒ‰ã‚¢ãƒƒãƒ—ç‡
        speedup = seq_avg_time / par_avg_time if par_avg_time > 0 else 0
        
        # åŠ¹ç‡æ€§ï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰åŠ¹ç‡ï¼‰
        thread_efficiency = speedup / self.config.get('n_threads', 4) * 100
        
        analysis = {
            'sequential_avg_time': seq_avg_time,
            'parallel_avg_time': par_avg_time,
            'speedup_ratio': speedup,
            'sequential_avg_tps': seq_avg_tps,
            'parallel_avg_tps': par_avg_tps,
            'thread_efficiency_percent': thread_efficiency,
            'performance_improvement_percent': ((seq_avg_time - par_avg_time) / seq_avg_time * 100) if seq_avg_time > 0 else 0,
            'meets_target': par_avg_time <= 2.0,  # ç›®æ¨™: 2ç§’ä»¥å†…
            'cpu_utilization_improvement': par_avg_tps / seq_avg_tps if seq_avg_tps > 0 else 0
        }
        
        return analysis
    
    def print_results(self, results: Dict[str, Any]) -> None:
        """
        ãƒ†ã‚¹ãƒˆçµæœã®è¡¨ç¤º
        
        å¼•æ•°:
            results: ãƒ†ã‚¹ãƒˆçµæœ
        """
        print("\n" + "="*80)
        print("MurmurNet CPUæœ€é©åŒ–ãƒ†ã‚¹ãƒˆçµæœ")
        print("="*80)
        
        comparison = results['comparison']
        
        print(f"\nğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ:")
        print(f"  é€æ¬¡å‡¦ç†å¹³å‡æ™‚é–“: {comparison['sequential_avg_time']:.2f}ç§’")
        print(f"  ä¸¦åˆ—å‡¦ç†å¹³å‡æ™‚é–“: {comparison['parallel_avg_time']:.2f}ç§’")
        print(f"  ã‚¹ãƒ”ãƒ¼ãƒ‰ã‚¢ãƒƒãƒ—ç‡: {comparison['speedup_ratio']:.2f}å€")
        print(f"  æ€§èƒ½å‘ä¸Šç‡: {comparison['performance_improvement_percent']:.1f}%")
        
        print(f"\nğŸš€ ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæ¯”è¼ƒ:")
        print(f"  é€æ¬¡å‡¦ç†: {comparison['sequential_avg_tps']:.1f} tokens/sec")
        print(f"  ä¸¦åˆ—å‡¦ç†: {comparison['parallel_avg_tps']:.1f} tokens/sec")
        print(f"  ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Š: {comparison['cpu_utilization_improvement']:.2f}å€")
        
        print(f"\nâš¡ CPUåŠ¹ç‡:")
        print(f"  ã‚¹ãƒ¬ãƒƒãƒ‰åŠ¹ç‡: {comparison['thread_efficiency_percent']:.1f}%")
        
        print(f"\nğŸ¯ ç›®æ¨™é”æˆçŠ¶æ³:")
        target_met = "âœ… é”æˆ" if comparison['meets_target'] else "âŒ æœªé”æˆ"
        print(f"  2ç§’ä»¥å†…å¿œç­”: {target_met} ({comparison['parallel_avg_time']:.2f}ç§’)")
        
        # è©³ç´°çµæœ
        print(f"\nğŸ“‹ è©³ç´°çµæœ:")
        print("  é€æ¬¡å‡¦ç†:")
        for i, result in enumerate(results['sequential'][:3]):  # æœ€åˆã®3ä»¶ã®ã¿è¡¨ç¤º
            if 'error' not in result:
                print(f"    ãƒ†ã‚¹ãƒˆ{i+1}: {result['execution_time']:.2f}ç§’ ({result['tokens_per_second']:.1f} tok/s)")
            else:
                print(f"    ãƒ†ã‚¹ãƒˆ{i+1}: ã‚¨ãƒ©ãƒ¼ - {result['error']}")
        
        print("  ä¸¦åˆ—å‡¦ç†:")
        for i, result in enumerate(results['parallel'][:3]):  # æœ€åˆã®3ä»¶ã®ã¿è¡¨ç¤º
            if 'error' not in result:
                print(f"    ãƒ†ã‚¹ãƒˆ{i+1}: {result['execution_time']:.2f}ç§’ ({result['tokens_per_second']:.1f} tok/s)")
            else:
                print(f"    ãƒ†ã‚¹ãƒˆ{i+1}: ã‚¨ãƒ©ãƒ¼ - {result['error']}")
        
        print("\n" + "="*80)

async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("MurmurNet CPUæœ€é©åŒ–ãƒ†ã‚¹ãƒˆã‚’é–‹å§‹ã—ã¾ã™...")
    
    try:
        # ãƒ†ã‚¹ã‚¿ãƒ¼ã®ä½œæˆ
        tester = CPUOptimizationTester()
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ
        results = await tester.run_performance_comparison()
        
        # çµæœã®è¡¨ç¤º
        tester.print_results(results)
        
        # çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        import json
        with open('cpu_optimization_test_results.json', 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“„ è©³ç´°çµæœã‚’ 'cpu_optimization_test_results.json' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
        
    except Exception as e:
        print(f"ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    # Windowsã§ã®éåŒæœŸå®Ÿè¡Œã®æœ€é©åŒ–
    if os.name == "nt":
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    
    asyncio.run(main())
