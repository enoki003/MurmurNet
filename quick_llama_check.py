#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
llama-cpp-pythonå˜ä½“å‹•ä½œç¢ºèªã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

try:
    from llama_cpp import Llama
    print("âœ… llama-cpp-python importæˆåŠŸ")
    
    model_path = r"C:\Users\admin\Desktop\èª²é¡Œç ”ç©¶\models\gemma-3-1b-it-q4_0.gguf"
    print(f"ğŸ“ ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹: {model_path}")
    
    m = Llama(model_path=model_path, n_ctx=2048, verbose=False)
    print("âœ… Llamaãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–æˆåŠŸ")
    
    response = m.create_chat_completion(
        messages=[{"role": "user", "content": "ping"}],
        max_tokens=4
    )
    print("âœ… create_chat_completionæˆåŠŸ")
    print("ğŸ“¤ ãƒ¬ã‚¹ãƒãƒ³ã‚¹:", response["choices"][0]["message"]["content"])
    
except Exception as e:
    print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
    import traceback
    traceback.print_exc()
