#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
MurmurNet Singleton LLMãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Llamaãƒ¢ãƒ‡ãƒ«ã®Singletonå®Ÿè£…
ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã¨ãƒ­ãƒ¼ãƒ‰æ™‚é–“ã‚’å¤§å¹…ã«å‰Šæ¸›

ä½œè€…: Yuhi Sonoki
"""

import logging
import os
import time
import threading
from functools import lru_cache
from typing import Dict, Any, Optional, Union, List
import psutil

logger = logging.getLogger('MurmurNet.ModelManager')

# æ¡ä»¶ä»˜ãã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from llama_cpp import Llama
    HAS_LLAMA_CPP = True
except ImportError:
    HAS_LLAMA_CPP = False
    Llama = None

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
_MODEL_INSTANCES = {}
_MODEL_LOCK = threading.Lock()

@lru_cache(maxsize=4)
def get_llama_model(model_path: str, 
                   n_ctx: int = 2048,
                   n_threads: int = 6,  # 4C/8T CPUã«æœ€é©åŒ–ï¼ˆç‰©ç†ã‚³ã‚¢+2ï¼‰
                   n_gpu_layers: int = 0,
                   chat_template: Optional[str] = None) -> Optional[object]:
    """
    Llamaãƒ¢ãƒ‡ãƒ«ã®Singletonå–å¾—
    
    @lru_cache ã«ã‚ˆã‚Šã€åŒã˜å¼•æ•°ã§ä½•åº¦å‘¼ã³å‡ºã•ã‚Œã¦ã‚‚
    åˆå›ã®ã¿ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã•ã‚Œã€ä»¥é™ã¯åŒã˜ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’è¿”ã™
    
    å¼•æ•°:
        model_path: ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        n_ctx: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚µã‚¤ã‚º
        n_threads: ã‚¹ãƒ¬ãƒƒãƒ‰æ•°
        n_gpu_layers: GPUå±¤æ•°
        chat_template: ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        
    æˆ»ã‚Šå€¤:
        Llamaã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ or None
    """
    if not HAS_LLAMA_CPP:
        logger.error("âŒ llama-cpp-python ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return None
    
    if not os.path.exists(model_path):
        logger.error(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_path}")
        return None
    
    try:
        logger.info(f"ğŸš€ Llamaãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–é–‹å§‹: {os.path.basename(model_path)}")
        start_time = time.time()
          # Llamaã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç”Ÿæˆ
        llama_kwargs = {
            'model_path': model_path,
            'n_ctx': n_ctx,
            'n_threads': n_threads,
            'n_gpu_layers': n_gpu_layers,
            'use_mmap': True,
            'use_mlock': True,
            'verbose': False,  # åˆæœŸåŒ–æ™‚ã®ãƒ­ã‚°ã‚’æŠ‘åˆ¶
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            'n_batch': 1024,  # 512â†’1024ã«å¢—åŠ ï¼ˆãƒãƒƒãƒå‡¦ç†åŠ¹ç‡å‘ä¸Šï¼‰
            'n_seq_max': 2,   # ä¸¦åˆ—ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æ•°ã‚’åˆ¶é™
            'last_n_tokens_size': 64,
            'rope_scaling_type': 0,
            'rope_freq_base': 10000.0,
            'rope_freq_scale': 1.0,
        }
        
        # ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®èª­ã¿è¾¼ã¿
        if chat_template and os.path.exists(chat_template):
            try:
                with open(chat_template, 'r', encoding='utf-8') as f:
                    llama_kwargs['chat_template'] = f.read()
            except Exception as e:
                logger.warning(f"âš ï¸ ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        
        # Llamaã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ
        llm = Llama(**llama_kwargs)
        
        load_time = time.time() - start_time
        logger.info(f"âœ… Llamaãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å®Œäº†: {os.path.basename(model_path)} ({load_time:.2f}s)")
        
        return llm
        
    except Exception as e:
        logger.error(f"âŒ Llamaãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        return None

class SingletonModelManager:
    """
    Singleton LLMãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
    
    å…¨ã¦ã®ModelFactoryã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã§åŒã˜Llamaãƒ¢ãƒ‡ãƒ«ã‚’å…±æœ‰
    ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã¨ãƒ­ãƒ¼ãƒ‰æ™‚é–“ã‚’å¤§å¹…ã«å‰Šæ¸›
    """
    
    def __init__(self, config: Dict[str, Any]):
        """
        Singleton ModelManagerã®åˆæœŸåŒ–
        
        å¼•æ•°:
            config: è¨­å®šè¾æ›¸
        """
        self.config = config
        self.model_path = config.get('model_path')
        self.n_ctx = config.get('n_ctx', 2048)
        self.n_threads = config.get('n_threads', 4)
        self.n_gpu_layers = config.get('n_gpu_layers', 0)
        self.chat_template = config.get('chat_template')
        self.temperature = config.get('temperature', 0.7)
        self.max_tokens = config.get('max_tokens', 256)
        self.debug = config.get('debug', False)
        
        if self.debug:
            logger.setLevel(logging.DEBUG)
    
    @property
    def model(self):
        """
        Llamaãƒ¢ãƒ‡ãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—
        
        åˆå›ã‚¢ã‚¯ã‚»ã‚¹æ™‚ã®ã¿ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã€ä»¥é™ã¯åŒã˜ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’è¿”ã™
        
        æˆ»ã‚Šå€¤:
            Llamaã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ or None
        """
        if not self.model_path:
            logger.error("âŒ model_pathãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return None
        
        return get_llama_model(
            self.model_path,
            self.n_ctx,
            self.n_threads,
            self.n_gpu_layers,
            self.chat_template
        )
    
    def generate(self, prompt: str, **kwargs) -> str:
        """
        ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
        
        å¼•æ•°:
            prompt: å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            **kwargs: ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            
        æˆ»ã‚Šå€¤:
            ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
        """
        model = self.model
        if model is None:
            return "âŒ Llamaãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“"
        
        try:
            # ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨­å®š
            generation_kwargs = {
                'max_tokens': kwargs.get('max_tokens', self.max_tokens),
                'temperature': kwargs.get('temperature', self.temperature),
                'top_p': kwargs.get('top_p', 0.9),
                'top_k': kwargs.get('top_k', 40),
                'repeat_penalty': kwargs.get('repeat_penalty', 1.1),
                'stop': kwargs.get('stop', []),
                'echo': False,
            }
            
            if self.debug:
                logger.debug(f"ğŸ¤– ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆé–‹å§‹: {len(prompt)}æ–‡å­—")
            
            # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆå®Ÿè¡Œ
            start_time = time.time()
            response = model(prompt, **generation_kwargs)
            generation_time = time.time() - start_time
            
            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å‡¦ç†
            if isinstance(response, dict) and 'choices' in response:
                if response['choices']:
                    if isinstance(response['choices'][0], dict):
                        text = response['choices'][0].get('text', '').strip()
                    else:
                        text = response['choices'][0].text.strip()
                else:
                    text = ""
            else:
                text = str(response).strip()
            
            if self.debug:
                tokens = len(text.split())
                speed = tokens / generation_time if generation_time > 0 else 0
                logger.debug(f"âœ… ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆå®Œäº†: {tokens}ãƒˆãƒ¼ã‚¯ãƒ³, {speed:.1f}tok/s")
            
            return text
            
        except Exception as e:
            logger.error(f"âŒ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return f"âŒ ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}"
    
    def create_chat_completion(self, messages: List[Dict[str, str]], **kwargs) -> Union[Dict[str, Any], object]:
        """
        ãƒãƒ£ãƒƒãƒˆå®Œäº†APIï¼ˆllama-cpp-pythonäº’æ›ï¼‰
        
        å¼•æ•°:
            messages: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆ [{"role": "user", "content": "..."}]
            **kwargs: ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            
        æˆ»ã‚Šå€¤:
            ãƒ¬ã‚¹ãƒãƒ³ã‚¹è¾æ›¸ã¾ãŸã¯ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ        """
        model = self.model
        if model is None:
            return {"error": "âŒ Llamaãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“"}
        
        try:
            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¤‰æ› (format_chat_promptã¯ä½¿ç”¨ã—ãªã„)
            prompt_parts = []
            for message in messages:
                role = message.get('role', 'user')
                content = message.get('content', '')
                if role == 'system':
                    prompt_parts.append(f"System: {content}")
                elif role == 'user':
                    prompt_parts.append(f"User: {content}")
                elif role == 'assistant':
                    prompt_parts.append(f"Assistant: {content}")
            prompt = "\n".join(prompt_parts) + "\nAssistant:"
              # ãƒãƒ£ãƒƒãƒˆå®Œäº†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨­å®š
            chat_kwargs = {
                'max_tokens': kwargs.get('max_tokens', self.max_tokens),
                'temperature': kwargs.get('temperature', self.temperature),
                'top_p': kwargs.get('top_p', 0.9),
                'top_k': kwargs.get('top_k', 40),
                'repeat_penalty': kwargs.get('repeat_penalty', 1.1),
                'stop': kwargs.get('stop', ['\n\n', 'User:', 'System:']),
                # 'echo': False  # create_chat_completionã§ã¯ä½¿ç”¨ã—ãªã„
            }
            
            if self.debug:
                logger.debug(f"ğŸ’¬ ãƒãƒ£ãƒƒãƒˆå®Œäº†é–‹å§‹: {len(messages)}ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸")
            
            # model.create_chat_completionãŒã‚ã‚‹å ´åˆã¯ãã‚Œã‚’ä½¿ç”¨
            if hasattr(model, 'create_chat_completion'):
                return model.create_chat_completion(messages=messages, **chat_kwargs)
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: é€šå¸¸ã®ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚’ä½¿ç”¨
                # é€šå¸¸ã®ç”Ÿæˆç”¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆechoã‚’è¿½åŠ ï¼‰
                generation_kwargs = dict(chat_kwargs)
                generation_kwargs['echo'] = False
                
                start_time = time.time()
                response = model(prompt, **generation_kwargs)
                generation_time = time.time() - start_time
                  # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å‡¦ç† - ã‚ˆã‚Šå …ç‰¢ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
                text = ""
                try:
                    if isinstance(response, dict):
                        if 'choices' in response and response['choices']:
                            # llama-cpp-pythonå½¢å¼ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹
                            choice = response['choices'][0]
                            if isinstance(choice, dict):
                                if 'text' in choice:
                                    text = choice['text'].strip()
                                elif 'message' in choice and 'content' in choice['message']:
                                    text = choice['message']['content'].strip()
                            else:
                                text = str(choice).strip()
                        else:
                            # ä»–ã®å½¢å¼ã®è¾æ›¸ãƒ¬ã‚¹ãƒãƒ³ã‚¹
                            text = str(response).strip()
                    else:
                        # æ–‡å­—åˆ—ã¾ãŸã¯ãã®ä»–ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹
                        text = str(response).strip()
                except Exception as parse_error:
                    logger.warning(f"ãƒ¬ã‚¹ãƒãƒ³ã‚¹è§£æã‚¨ãƒ©ãƒ¼: {parse_error}")
                    text = str(response).strip() if response else ""
                
                result = {
                    "id": f"chatcmpl-{int(time.time())}",
                    "object": "chat.completion",
                    "created": int(time.time()),
                    "model": self.model_path,
                    "choices": [{
                        "index": 0,
                        "message": {
                            "role": "assistant",
                            "content": text
                        },
                        "finish_reason": "stop"
                    }],
                    "usage": {
                        "prompt_tokens": len(prompt.split()),
                        "completion_tokens": len(text.split()),
                        "total_tokens": len(prompt.split()) + len(text.split())
                    }
                }
                
                if self.debug:
                    tokens = len(text.split())
                    speed = tokens / generation_time if generation_time > 0 else 0
                    logger.debug(f"âœ… ãƒãƒ£ãƒƒãƒˆå®Œäº†: {tokens}ãƒˆãƒ¼ã‚¯ãƒ³, {speed:.1f}tok/s")
                
                return result
                
        except Exception as e:
            logger.error(f"âŒ ãƒãƒ£ãƒƒãƒˆå®Œäº†ã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": f"âŒ ãƒãƒ£ãƒƒãƒˆå®Œäº†ã‚¨ãƒ©ãƒ¼: {str(e)}"}

    def is_available(self) -> bool:
        """
        ãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
        
        æˆ»ã‚Šå€¤:
            True: åˆ©ç”¨å¯èƒ½, False: åˆ©ç”¨ä¸å¯
        """
        return self.model is not None
    
    def get_model_info(self) -> Dict[str, Any]:
        """
        ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’å–å¾—
        
        æˆ»ã‚Šå€¤:
            ãƒ¢ãƒ‡ãƒ«æƒ…å ±è¾æ›¸
        """
        return {
            'model_path': self.model_path,
            'n_ctx': self.n_ctx,
            'n_threads': self.n_threads,
            'n_gpu_layers': self.n_gpu_layers,
            'available': self.is_available(),
            'memory_usage': self._get_memory_usage()
        }
    
    def _get_memory_usage(self) -> float:
        """
        ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å–å¾—ï¼ˆMBï¼‰
        
        æˆ»ã‚Šå€¤:
            ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆMBï¼‰
        """
        try:
            process = psutil.Process()
            return process.memory_info().rss / 1024 / 1024
        except Exception:
            return 0.0

# ä¾¿åˆ©é–¢æ•°
def get_singleton_manager(config: Dict[str, Any]) -> SingletonModelManager:
    """
    Singleton ModelManagerã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—
    
    å¼•æ•°:
        config: è¨­å®šè¾æ›¸
        
    æˆ»ã‚Šå€¤:
        SingletonModelManagerã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
    """
    return SingletonModelManager(config)

# å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹
def get_shared_model_manager(config: Dict[str, Any]) -> SingletonModelManager:
    """å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹"""
    return get_singleton_manager(config)

if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆç”¨ã‚³ãƒ¼ãƒ‰
    import time
    
    logging.basicConfig(level=logging.INFO)
    
    # ãƒ†ã‚¹ãƒˆè¨­å®šï¼ˆå®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã«å¤‰æ›´ã—ã¦ãã ã•ã„ï¼‰
    config = {
        'model_path': 'path/to/your/model.gguf',
        'n_ctx': 2048,
        'n_threads': 4,
        'debug': True
    }
    
    print("=== SingletonModelManager ãƒ†ã‚¹ãƒˆ ===")
    
    # 1å›ç›®ã®ãƒ­ãƒ¼ãƒ‰
    manager1 = get_singleton_manager(config)
    start_time = time.time()
    available1 = manager1.is_available()
    time1 = time.time() - start_time
    print(f"1å›ç›®: {time1:.3f}s, åˆ©ç”¨å¯èƒ½: {available1}")
    
    # 2å›ç›®ã®ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ï¼‰
    manager2 = get_singleton_manager(config)
    start_time = time.time()
    available2 = manager2.is_available()
    time2 = time.time() - start_time
    print(f"2å›ç›®: {time2:.3f}s, åˆ©ç”¨å¯èƒ½: {available2}")
    
    # åŒã˜ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‹ãƒã‚§ãƒƒã‚¯
    print(f"åŒã˜modelã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹: {manager1.model is manager2.model}")
    if time1 > 0 and time2 > 0:
        print(f"é€Ÿåº¦å‘ä¸Š: {time1/time2:.1f}å€")
