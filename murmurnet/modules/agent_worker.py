#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
MurmurNet ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ¯ãƒ¼ã‚«ãƒ¼
~~~~~~~~~~~~~~~~~~~~~~~~~~~~
ProcessPoolExecutorã«ã‚ˆã‚‹çœŸã®ä¸¦åˆ—ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè¡Œ
GILåˆ¶ç´„ã‚’å›é¿ã—ã€çœŸã®ä¸¦åˆ—å‡¦ç†ã‚’å®Ÿç¾

ä½œè€…: Yuhi Sonoki
"""

import logging
import os
import time
import threading
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, as_completed
from typing import Dict, Any, List, Optional, Tuple, Callable
import psutil
import re

logger = logging.getLogger('MurmurNet.AgentWorker')

def _clean_response(response: str) -> str:
    """
    ãƒ¢ãƒ‡ãƒ«ã®å¿œç­”ã‚’æ¸…æµ„åŒ–ã™ã‚‹
    
    å¼•æ•°:
        response: ç”Ÿã®å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆ
        
    æˆ»ã‚Šå€¤:
        æ¸…æµ„åŒ–ã•ã‚ŒãŸå¿œç­”ãƒ†ã‚­ã‚¹ãƒˆ
    """
    if not response:
        return ""
    
    # æ”¹è¡Œã‚’æ­£è¦åŒ–
    response = response.strip()
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæŒ‡ç¤ºã®é™¤å»ï¼ˆç·©å’Œç‰ˆï¼šå®Ÿéš›ã®å›ç­”å†…å®¹ã‚’ä¿æŒï¼‰
    prompt_patterns = [
        r'System:\s*.*?\n\nUser:\s*.*?\n\nAssistant:\s*',  # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå…¨ä½“
        r'^å›ç­”[ï¼š:]\s*',  # å†’é ­ã®ã€Œå›ç­”:ã€ã®ã¿
        r'^\s*å¿œç­”[ï¼š:]\s*',  # å†’é ­ã®ã€Œå¿œç­”:ã€ã®ã¿
    ]
    
    for pattern in prompt_patterns:
        response = re.sub(pattern, '', response, flags=re.DOTALL | re.IGNORECASE)
    
    # é‡è¤‡ã—ãŸãƒ†ã‚­ã‚¹ãƒˆã®é™¤å»ï¼ˆåŒã˜æ–‡ãŒ2å›ä»¥ä¸Šç¶šãå ´åˆï¼‰
    sentences = response.split('ã€‚')
    cleaned_sentences = []
    prev_sentence = ""
    
    for sentence in sentences:
        sentence = sentence.strip()
        if sentence and sentence != prev_sentence:
            cleaned_sentences.append(sentence)
            prev_sentence = sentence
    
    response = 'ã€‚'.join(cleaned_sentences)
    if response and not response.endswith('ã€‚'):
        response += 'ã€‚'
    
    # é•·ã•ã‚’åˆ¶é™ï¼ˆ1000æ–‡å­—ï¼šç©ºãƒ¬ã‚¹å¯¾ç­–ã§åˆ¶é™ç·©å’Œï¼‰
    if len(response) > 1000:
        # æ–‡ã®å¢ƒç•Œã§åˆ‡æ–­
        sentences = response.split('ã€‚')
        truncated = ""
        for sentence in sentences:
            if len(truncated + sentence + 'ã€‚') <= 1000:
                truncated += sentence + 'ã€‚'
            else:
                break
        response = truncated if truncated else response[:997] + "..."
    
    # ç©ºãƒ¬ã‚¹å¯¾ç­–ï¼šç©ºã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    if not response or len(response.strip()) < 3:
        return "(no content)"
    
    return response.strip()

def worker_process_function(agent_config: Dict[str, Any],
                          agent_id: int, 
                          input_data: Any,
                          rag_data: Any,
                          context_data: Any) -> Tuple[int, str, Dict[str, Any]]:
    """
    ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ã§å®Ÿè¡Œã•ã‚Œã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–¢æ•°
    
    å„ãƒ—ãƒ­ã‚»ã‚¹ã§ç‹¬ç«‹ã—ã¦Llamaãƒ¢ãƒ‡ãƒ«ã¨Embedderã‚’åˆæœŸåŒ–ã—ã€
    çœŸã®ä¸¦åˆ—å‡¦ç†ã‚’å®Ÿç¾
    
    å¼•æ•°:
        agent_config: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¨­å®š
        agent_id: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆID
        input_data: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
        rag_data: RAGãƒ‡ãƒ¼ã‚¿
        context_data: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
        
    æˆ»ã‚Šå€¤:
        (agent_id, response, stats)
    """
    # ãƒ—ãƒ­ã‚»ã‚¹å†…ã§ã®ãƒ­ã‚°è¨­å®š
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    worker_logger = logging.getLogger(f'MurmurNet.Worker.{agent_id}')
    
    start_time = time.time()
    process_id = os.getpid()
    thread_id = threading.get_native_id() if hasattr(threading, 'get_native_id') else threading.get_ident()
    
    worker_logger.info(f"ğŸš€ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ{agent_id} ãƒ—ãƒ­ã‚»ã‚¹{process_id} ã‚¹ãƒ¬ãƒƒãƒ‰{thread_id} é–‹å§‹")
    
    try:
        # ãƒ—ãƒ­ã‚»ã‚¹å†…ã§llama-cpp-pythonã®å¯ç”¨æ€§ã‚’ç¢ºèª
        try:
            from llama_cpp import Llama
            has_llama_cpp = True
        except ImportError as e:
            worker_logger.error(f"âŒ ãƒ—ãƒ­ã‚»ã‚¹{process_id}ã§llama-cpp-pythonã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            has_llama_cpp = False
            
        if not has_llama_cpp:
            return {
                'agent_id': agent_id,
                'success': False,
                'output': "llama-cpp-pythonãŒåˆ©ç”¨ã§ãã¾ã›ã‚“",
                'error': "llama-cpp-pythonã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼",
                'stats': {'process_id': process_id, 'thread_id': thread_id, 'total_time': 0}
            }
        
        # ãƒ—ãƒ­ã‚»ã‚¹å†…ã§ãƒ¢ãƒ‡ãƒ«ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’åˆæœŸåŒ–
        from MurmurNet.modules.model_manager import get_singleton_manager
        
        model_manager = get_singleton_manager(agent_config)
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ§‹ç¯‰
        role = agent_config.get('roles', [{}])[agent_id] if agent_id < len(agent_config.get('roles', [])) else {}
        role_name = role.get('role', f"ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ{agent_id}")
        
        input_text = str(input_data) if input_data else ""
        rag_text = str(rag_data)[:300] if rag_data else ""
        context_text = str(context_data)[:200] if context_data else ""
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®æ§‹ç¯‰
        context = ""
        if rag_text:
            context += f"å‚è€ƒæƒ…å ±: {rag_text}\n"
        if context_text:
            context += f"ä¼šè©±å±¥æ­´: {context_text}"
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’ä½¿ç”¨ã—ã¦HuggingFaceå½¢å¼ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
        from MurmurNet.modules.prompt_manager import get_prompt_manager
        
        # ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã¨ãƒ¢ãƒ‡ãƒ«åã‚’å–å¾—
        model_type = agent_config.get('model_type', 'llama')
        model_name = agent_config.get('huggingface_model_name', '') if model_type == 'huggingface' else ''
        
        if model_type == 'huggingface':
            prompt_manager = get_prompt_manager('huggingface', model_name)
            prompt = prompt_manager.build_prompt(input_text, role_name, context.strip())
        else:
            # Llamaãƒ¢ãƒ‡ãƒ«ã®å ´åˆã¯å¾“æ¥ã®å½¢å¼ã‚’ç¶­æŒ
            system_prompt = f"""ã‚ãªãŸã¯ã€Œ{role_name}ã€ã¨ã—ã¦è¡Œå‹•ã—ã¦ãã ã•ã„ã€‚

ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦å›ç­”ã—ã¦ãã ã•ã„ï¼š
- 150æ–‡å­—ä»¥å†…ã§ç°¡æ½”ã«å›ç­”ã™ã‚‹
- è³ªå•ã«ç›´æ¥ç­”ãˆã‚‹
- é–¢é€£æƒ…å ±ã¨ä¼šè©±å±¥æ­´ã‚’å‚è€ƒã«ã™ã‚‹
- è‡ªç„¶ã§èª­ã¿ã‚„ã™ã„æ—¥æœ¬èªã§ç­”ãˆã‚‹"""

            user_prompt = f"""è³ªå•: {input_text}

{context}

å›ç­”:"""

            prompt = f"System: {system_prompt}\n\nUser: {user_prompt}\n\nAssistant:"
        
        # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆå®Ÿè¡Œ
        worker_logger.debug(f"ğŸ¤– ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆé–‹å§‹: {len(prompt)}æ–‡å­—")
        generation_start = time.time()
        
        try:
            response = model_manager.generate(
                prompt,
                max_tokens=agent_config.get('max_tokens', 256),
                temperature=agent_config.get('temperature', 0.7)
            )
            generation_time = time.time() - generation_start
            worker_logger.debug(f"âœ… ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆå®Œäº†: {len(response.split())}ãƒˆãƒ¼ã‚¯ãƒ³, {len(response.split())/generation_time:.1f}tok/s")
        except Exception as gen_error:
            generation_time = time.time() - generation_start
            worker_logger.error(f"âŒ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {gen_error} ({generation_time:.2f}s)")
            raise gen_error
        
        # å¿œç­”ã®å¾Œå‡¦ç†
        response = _clean_response(response)
        
        total_time = time.time() - start_time
        
        # çµ±è¨ˆæƒ…å ±
        stats = {
            'process_id': process_id,
            'thread_id': thread_id,
            'total_time': total_time,
            'generation_time': generation_time,
            'response_length': len(response),
            'tokens_estimated': len(response.split()),
            'tokens_per_second': len(response.split()) / generation_time if generation_time > 0 else 0
        }
        
        worker_logger.info(f"âœ… ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ{agent_id} å®Œäº†: {total_time:.2f}s, {stats['tokens_per_second']:.1f}tok/s")
        
        return agent_id, response, stats
        
    except Exception as e:
        error_time = time.time() - start_time
        error_msg = str(e) if str(e) else f"Unknown error: {type(e).__name__}"
        worker_logger.error(f"âŒ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ{agent_id} ã‚¨ãƒ©ãƒ¼: {error_msg} ({error_time:.2f}s)")
        
        # è©³ç´°ãªãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯ã‚’å¸¸ã«å‡ºåŠ›
        import traceback
        worker_logger.error(f"ã‚¨ãƒ©ãƒ¼ã®ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯:\n{traceback.format_exc()}")
        
        stats = {
            'process_id': process_id,
            'thread_id': thread_id,
            'total_time': error_time,
            'generation_time': 0,
            'response_length': 0,
            'tokens_estimated': 0,
            'tokens_per_second': 0,
            'error': error_msg
        }
        
        return agent_id, f"ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ{agent_id}ã¯å¿œç­”ã§ãã¾ã›ã‚“ã§ã—ãŸ: {e}", stats

def _clean_response(response: str) -> str:
    """
    ãƒ¢ãƒ‡ãƒ«ã®å¿œç­”ã‚’æ¸…æµ„åŒ–ã™ã‚‹
    
    å¼•æ•°:
        response: ç”Ÿã®å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆ
        
    æˆ»ã‚Šå€¤:
        æ¸…æµ„åŒ–ã•ã‚ŒãŸå¿œç­”ãƒ†ã‚­ã‚¹ãƒˆ
    """
    if not response:
        return ""
    
    # æ”¹è¡Œã‚’æ­£è¦åŒ–
    response = response.strip()
    
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæŒ‡ç¤ºã®é™¤å»ï¼ˆç·©å’Œç‰ˆï¼šå®Ÿéš›ã®å›ç­”å†…å®¹ã‚’ä¿æŒï¼‰
    prompt_patterns = [
        r'System:\s*.*?\n\nUser:\s*.*?\n\nAssistant:\s*',  # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå…¨ä½“
        r'^å›ç­”[ï¼š:]\s*',  # å†’é ­ã®ã€Œå›ç­”:ã€ã®ã¿
        r'^\s*å¿œç­”[ï¼š:]\s*',  # å†’é ­ã®ã€Œå¿œç­”:ã€ã®ã¿
    ]
    
    for pattern in prompt_patterns:
        response = re.sub(pattern, '', response, flags=re.DOTALL | re.IGNORECASE)
    
    # é‡è¤‡ã—ãŸãƒ†ã‚­ã‚¹ãƒˆã®é™¤å»ï¼ˆåŒã˜æ–‡ãŒ2å›ä»¥ä¸Šç¶šãå ´åˆï¼‰
    sentences = response.split('ã€‚')
    cleaned_sentences = []
    prev_sentence = ""
    
    for sentence in sentences:
        sentence = sentence.strip()
        if sentence and sentence != prev_sentence:
            cleaned_sentences.append(sentence)
            prev_sentence = sentence
    
    response = 'ã€‚'.join(cleaned_sentences)
    if response and not response.endswith('ã€‚'):
        response += 'ã€‚'
    
    # é•·ã•ã‚’åˆ¶é™ï¼ˆ1000æ–‡å­—ï¼šç©ºãƒ¬ã‚¹å¯¾ç­–ã§åˆ¶é™ç·©å’Œï¼‰
    if len(response) > 1000:
        # æ–‡ã®å¢ƒç•Œã§åˆ‡æ–­
        sentences = response.split('ã€‚')
        truncated = ""
        for sentence in sentences:
            if len(truncated + sentence + 'ã€‚') <= 1000:
                truncated += sentence + 'ã€‚'
            else:
                break
        response = truncated if truncated else response[:997] + "..."

    # ç©ºãƒ¬ã‚¹å¯¾ç­–ï¼šç©ºã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    if not response or len(response.strip()) < 3:
        return "(no content)"

    return response.strip()

class ProcessParallelAgentWorker:
    """
    ProcessPoolExecutorã«ã‚ˆã‚‹çœŸã®ä¸¦åˆ—ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè¡Œ
    
    ThreadPoolExecutorã®ä»£ã‚ã‚Šã«ProcessPoolExecutorã‚’ä½¿ç”¨ã—ã€
    GILåˆ¶ç´„ã‚’å›é¿ã—ã¦çœŸã®ä¸¦åˆ—å‡¦ç†ã‚’å®Ÿç¾
    """
    
    def __init__(self, config: Dict[str, Any]):
        """
        ProcessParallelAgentWorkerã®åˆæœŸåŒ–
        
        å¼•æ•°:
            config: è¨­å®šè¾æ›¸
        """
        self.config = config
        self.debug = config.get('debug', False)
        self.num_agents = config.get('num_agents', 2)
        
        # CPUæœ€é©åŒ–è¨­å®š
        self.cpu_count = psutil.cpu_count(logical=False) or 4  # ç‰©ç†ã‚³ã‚¢æ•°
        self.max_workers = min(self.num_agents, self.cpu_count)
        
        # ProcessPoolExecutorã®åˆæœŸåŒ–
        self.process_pool = ProcessPoolExecutor(
            max_workers=self.max_workers,
            mp_context=mp.get_context('spawn')  # Windowså¯¾å¿œ
        )
        
        if self.debug:
            logger.setLevel(logging.DEBUG)
        
        logger.info(f"ğŸš€ ProcessParallelAgentWorkeråˆæœŸåŒ–: {self.max_workers}ãƒ—ãƒ­ã‚»ã‚¹, {self.num_agents}ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ")
    
    def execute_agents_parallel(self, 
                              input_data: Any,
                              rag_data: Any = None,
                              context_data: Any = None) -> Dict[str, Any]:
        """
        è¤‡æ•°ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ä¸¦åˆ—å®Ÿè¡Œ
        
        å¼•æ•°:
            input_data: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
            rag_data: RAGãƒ‡ãƒ¼ã‚¿
            context_data: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
            
        æˆ»ã‚Šå€¤:
            å®Ÿè¡Œçµæœè¾æ›¸
        """
        start_time = time.time()
        
        logger.info(f"ğŸ¤– {self.num_agents}ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä¸¦åˆ—å®Ÿè¡Œé–‹å§‹")
        
        # å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã‚¿ã‚¹ã‚¯ã‚’é€ä¿¡
        futures = []
        for agent_id in range(self.num_agents):
            future = self.process_pool.submit(
                worker_process_function,
                self.config,
                agent_id,
                input_data,
                rag_data,
                context_data
            )
            futures.append((agent_id, future))        # çµæœã‚’åé›†
        results = {}
        all_stats = []
        
        for agent_id, future in futures:
            try:
                returned_id, response, stats = future.result(timeout=60)  # 60ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã«å»¶é•·
                results[f'agent_{returned_id}_output'] = response
                all_stats.append(stats)
                
                if self.debug:
                    logger.debug(f"âœ… ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ{returned_id} çµæœå–å¾—: {stats['tokens_per_second']:.1f}tok/s")
                    
            except Exception as e:
                error_msg = str(e) if str(e) else f"Unknown error: {type(e).__name__}"
                logger.error(f"âŒ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ{agent_id} å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {error_msg}")
                if self.debug:
                    import traceback
                    logger.debug(f"ã‚¨ãƒ©ãƒ¼ã®è©³ç´°:\n{traceback.format_exc()}")
                else:
                    # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã§ãªã„å ´åˆã‚‚æœ€ä½é™ã®ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯ã‚’å‡ºåŠ›
                    import traceback
                    logger.error(f"ã‚¨ãƒ©ãƒ¼ã®ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯: {traceback.format_exc()}")
                    
                results[f'agent_{agent_id}_output'] = f"ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ{agent_id}ã¯å¿œç­”ã§ãã¾ã›ã‚“ã§ã—ãŸ"
                all_stats.append({
                    'process_id': 0,
                    'thread_id': 0,
                    'total_time': 0,
                    'generation_time': 0,
                    'response_length': 0,
                    'tokens_estimated': 0,
                    'tokens_per_second': 0,
                    'error': error_msg
                })
        
        total_time = time.time() - start_time
        
        # çµ±è¨ˆè¨ˆç®—
        total_tokens = sum(stat['tokens_estimated'] for stat in all_stats)
        total_generation_time = sum(stat['generation_time'] for stat in all_stats)
        average_tokens_per_second = total_tokens / total_generation_time if total_generation_time > 0 else 0
        
        # ãƒ—ãƒ­ã‚»ã‚¹ä¸¦åˆ—æ€§ã®ç¢ºèª
        unique_processes = len(set(stat['process_id'] for stat in all_stats if stat['process_id'] > 0))
        
        parallel_stats = {
            'total_execution_time': total_time,
            'total_tokens': total_tokens,
            'average_tokens_per_second': average_tokens_per_second,
            'unique_processes': unique_processes,
            'parallel_efficiency': unique_processes / self.num_agents if self.num_agents > 0 else 0,
            'agent_stats': all_stats
        }
        
        results['parallel_stats'] = parallel_stats
        
        logger.info(f"âœ… ä¸¦åˆ—å®Ÿè¡Œå®Œäº†: {total_time:.2f}s, {average_tokens_per_second:.1f}tok/s, {unique_processes}ãƒ—ãƒ­ã‚»ã‚¹ä½¿ç”¨")
        
        return results
    
    def execute_agents_sequential(self,
                                input_data: Any,
                                rag_data: Any = None,
                                context_data: Any = None) -> Dict[str, Any]:
        """
        ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®é€æ¬¡å®Ÿè¡Œï¼ˆæ¯”è¼ƒç”¨ï¼‰
        
        å¼•æ•°:
            input_data: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
            rag_data: RAGãƒ‡ãƒ¼ã‚¿
            context_data: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
            
        æˆ»ã‚Šå€¤:
            å®Ÿè¡Œçµæœè¾æ›¸
        """
        start_time = time.time()
        
        logger.info(f"ğŸŒ {self.num_agents}ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé€æ¬¡å®Ÿè¡Œé–‹å§‹")
        
        results = {}
        all_stats = []
        
        for agent_id in range(self.num_agents):
            try:
                returned_id, response, stats = worker_process_function(
                    self.config, agent_id, input_data, rag_data, context_data
                )
                results[f'agent_{returned_id}_output'] = response
                all_stats.append(stats)
                
            except Exception as e:
                logger.error(f"âŒ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ{agent_id} é€æ¬¡å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
                results[f'agent_{agent_id}_output'] = f"ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ{agent_id}ã¯å¿œç­”ã§ãã¾ã›ã‚“ã§ã—ãŸ"
                all_stats.append({
                    'process_id': os.getpid(),
                    'thread_id': threading.get_native_id() if hasattr(threading, 'get_native_id') else threading.get_ident(),
                    'total_time': 0,
                    'generation_time': 0,
                    'response_length': 0,
                    'tokens_estimated': 0,
                    'tokens_per_second': 0,
                    'error': str(e)
                })
        
        total_time = time.time() - start_time
        
        # çµ±è¨ˆè¨ˆç®—
        total_tokens = sum(stat['tokens_estimated'] for stat in all_stats)
        total_generation_time = sum(stat['generation_time'] for stat in all_stats)
        average_tokens_per_second = total_tokens / total_generation_time if total_generation_time > 0 else 0
        
        sequential_stats = {
            'total_execution_time': total_time,
            'total_tokens': total_tokens,
            'average_tokens_per_second': average_tokens_per_second,
            'unique_processes': 1,
            'parallel_efficiency': 0,
            'agent_stats': all_stats
        }
        
        results['sequential_stats'] = sequential_stats
        
        logger.info(f"âœ… é€æ¬¡å®Ÿè¡Œå®Œäº†: {total_time:.2f}s, {average_tokens_per_second:.1f}tok/s")
        
        return results
    
    def shutdown(self):
        """
        ProcessPoolExecutorã®ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³
        """
        if hasattr(self, 'process_pool') and self.process_pool:
            logger.info("ğŸ”„ ProcessPoolExecutorã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³ä¸­...")
            try:
                self.process_pool.shutdown(wait=True, timeout=10)
                logger.info("âœ… ProcessPoolExecutorã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³å®Œäº†")
            except Exception as e:
                logger.warning(f"âš ï¸ ProcessPoolExecutorã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³ã‚¨ãƒ©ãƒ¼: {e}")
            finally:
                self.process_pool = None
    
    def __enter__(self):
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.shutdown()

# ä¾¿åˆ©é–¢æ•°
def create_process_parallel_worker(config: Dict[str, Any]) -> ProcessParallelAgentWorker:
    """
    ProcessParallelAgentWorkerã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
    
    å¼•æ•°:
        config: è¨­å®šè¾æ›¸
        
    æˆ»ã‚Šå€¤:
        ProcessParallelAgentWorkerã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
    """
    return ProcessParallelAgentWorker(config)

if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆç”¨ã‚³ãƒ¼ãƒ‰
    import json
    
    logging.basicConfig(level=logging.INFO)
    
    # ãƒ†ã‚¹ãƒˆè¨­å®š
    config = {
        'num_agents': 2,
        'model_path': 'path/to/your/model.gguf',  # å®Ÿéš›ã®ãƒ‘ã‚¹ã«å¤‰æ›´
        'n_ctx': 2048,
        'n_threads': 4,
        'max_tokens': 100,
        'temperature': 0.7,
        'debug': True,
        'roles': [
            {'role': 'ã‚¢ãƒŠãƒªã‚¹ãƒˆ', 'system': 'ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚'},
            {'role': 'ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼', 'system': 'åŠ©è¨€ã‚’æä¾›ã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚'}
        ]
    }
    
    print("=== ProcessParallelAgentWorker ãƒ†ã‚¹ãƒˆ ===")
    
    with create_process_parallel_worker(config) as worker:
        # ä¸¦åˆ—å®Ÿè¡Œãƒ†ã‚¹ãƒˆ
        print("\n--- ä¸¦åˆ—å®Ÿè¡Œ ---")
        parallel_results = worker.execute_agents_parallel(
            input_data="AIã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„",
            rag_data="AIã¯äººå·¥çŸ¥èƒ½ã®ç•¥ã§ã™",
            context_data="æŠ€è¡“çš„ãªè³ªå•ã‚’ã—ã¦ã„ã¾ã™"
        )
        
        print(f"ä¸¦åˆ—å®Ÿè¡Œçµæœ:")
        for key, value in parallel_results.items():
            if 'agent_' in key and 'output' in key:
                print(f"  {key}: {value[:100]}...")
        
        if 'parallel_stats' in parallel_results:
            stats = parallel_results['parallel_stats']
            print(f"  å®Ÿè¡Œæ™‚é–“: {stats['total_execution_time']:.2f}s")
            print(f"  ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {stats['average_tokens_per_second']:.1f}tok/s")
            print(f"  ä½¿ç”¨ãƒ—ãƒ­ã‚»ã‚¹æ•°: {stats['unique_processes']}")
            print(f"  ä¸¦åˆ—åŠ¹ç‡: {stats['parallel_efficiency']:.2f}")
        
        # é€æ¬¡å®Ÿè¡Œãƒ†ã‚¹ãƒˆ
        print("\n--- é€æ¬¡å®Ÿè¡Œ ---")
        sequential_results = worker.execute_agents_sequential(
            input_data="AIã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„", 
            rag_data="AIã¯äººå·¥çŸ¥èƒ½ã®ç•¥ã§ã™",
            context_data="æŠ€è¡“çš„ãªè³ªå•ã‚’ã—ã¦ã„ã¾ã™"
        )
        
        if 'sequential_stats' in sequential_results:
            stats = sequential_results['sequential_stats']
            print(f"  å®Ÿè¡Œæ™‚é–“: {stats['total_execution_time']:.2f}s")
            print(f"  ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {stats['average_tokens_per_second']:.1f}tok/s")
