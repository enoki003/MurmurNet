#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Simple Embedder ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
~~~~~~~~~~~~~~~~~~~~~~~~
SentenceTransformerä¾å­˜ãªã—ã®è»½é‡åŸ‹ã‚è¾¼ã¿å®Ÿè£…
å˜èªãƒ™ãƒ¼ã‚¹ãƒ»TF-IDFãƒ»ãƒãƒƒã‚·ãƒ¥ãƒ™ãƒ¼ã‚¹ã®åŸ‹ã‚è¾¼ã¿æ‰‹æ³•

æ©Ÿèƒ½:
- TF-IDF ãƒ™ã‚¯ãƒˆãƒ«åŒ–
- å˜èªé »åº¦ãƒ™ãƒ¼ã‚¹åŸ‹ã‚è¾¼ã¿
- ãƒãƒƒã‚·ãƒ¥ãƒ™ãƒ¼ã‚¹é«˜é€ŸåŸ‹ã‚è¾¼ã¿
- ã‚ªãƒ•ãƒ©ã‚¤ãƒ³å®Œå…¨å‹•ä½œ

ä½œè€…: Yuhi Sonoki
"""

import logging
import re
import numpy as np
from typing import List, Union, Optional, Dict, Any
from collections import Counter
import hashlib

logger = logging.getLogger(__name__)

class SimpleEmbedder:
    """
    ã‚·ãƒ³ãƒ—ãƒ«ãªåŸ‹ã‚è¾¼ã¿å®Ÿè£…
    
    SentenceTransformerä¸è¦ã®è»½é‡åŸ‹ã‚è¾¼ã¿æ‰‹æ³•
    """
    
    def __init__(self, config: Dict[str, Any] = None):
        """
        SimpleEmbedderåˆæœŸåŒ–
        
        Args:
            config: è¨­å®šè¾æ›¸
                - embedding_dim: åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒæ•° (default: 384)
                - method: åŸ‹ã‚è¾¼ã¿æ‰‹æ³• ('tfidf', 'bow', 'hash') (default: 'hash')
                - vocab_size: èªå½™ã‚µã‚¤ã‚º (default: 10000)
        """
        self.config = config or {}
        self.embedding_dim = self.config.get('embedding_dim', 384)
        self.method = self.config.get('embedding_method', 'hash')
        self.vocab_size = self.config.get('vocab_size', 10000)
        
        # èªå½™è¾æ›¸ï¼ˆTF-IDFç”¨ï¼‰
        self.vocab = {}
        self.idf_scores = {}
        self.is_fitted = False
        
        logger.info(f"SimpleEmbedderåˆæœŸåŒ–: method={self.method}, dim={self.embedding_dim}")
    
    def _tokenize(self, text: str) -> List[str]:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        
        Args:
            text: å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            ãƒˆãƒ¼ã‚¯ãƒ³ãƒªã‚¹ãƒˆ
        """
        if not text:
            return []
        
        # ç°¡å˜ãªãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼ˆå˜èªåˆ†å‰²ï¼‰
        text = text.lower()
        text = re.sub(r'[^\w\s]', ' ', text)  # å¥èª­ç‚¹é™¤å»
        tokens = text.split()
        return [token for token in tokens if len(token) > 1]  # 1æ–‡å­—ã®å˜èªé™¤å»
    
    def _hash_embedding(self, text: str) -> np.ndarray:
        """
        ãƒãƒƒã‚·ãƒ¥ãƒ™ãƒ¼ã‚¹é«˜é€ŸåŸ‹ã‚è¾¼ã¿
        
        Args:
            text: å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«
        """
        # ãƒ†ã‚­ã‚¹ãƒˆã®ãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—
        text_hash = hashlib.md5(text.encode('utf-8')).hexdigest()
        
        # ãƒãƒƒã‚·ãƒ¥ã‚’æ•°å€¤ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›
        hash_values = []
        for i in range(0, len(text_hash), 2):
            hash_values.append(int(text_hash[i:i+2], 16))
        
        # æ¬¡å…ƒæ•°ã«åˆã‚ã›ã¦èª¿æ•´
        embedding = np.zeros(self.embedding_dim)
        for i, val in enumerate(hash_values):
            if i < self.embedding_dim:
                embedding[i] = (val - 128) / 128.0  # -1 to 1 ã«æ­£è¦åŒ–
        
        # æ®‹ã‚Šã®æ¬¡å…ƒã¯æ–‡å­—åˆ—é•·ã‚„ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã§åŸ‹ã‚ã‚‹
        tokens = self._tokenize(text)
        for i in range(len(hash_values), self.embedding_dim):
            if i % 3 == 0:
                embedding[i] = len(text) / 1000.0  # æ–‡å­—åˆ—é•·
            elif i % 3 == 1:
                embedding[i] = len(tokens) / 100.0  # ãƒˆãƒ¼ã‚¯ãƒ³æ•°
            else:
                embedding[i] = np.random.normal(0, 0.1)  # ãƒã‚¤ã‚º
        
        # L2æ­£è¦åŒ–
        norm = np.linalg.norm(embedding)
        if norm > 0:
            embedding = embedding / norm
        
        return embedding
    
    def _bow_embedding(self, text: str) -> np.ndarray:
        """
        Bag of Words åŸ‹ã‚è¾¼ã¿
        
        Args:
            text: å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«
        """
        tokens = self._tokenize(text)
        if not tokens:
            return np.zeros(self.embedding_dim)
        
        # ãƒˆãƒ¼ã‚¯ãƒ³é »åº¦è¨ˆç®—
        token_counts = Counter(tokens)
        
        # ãƒãƒƒã‚·ãƒ¥ãƒ™ãƒ¼ã‚¹ã§ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æ¬¡å…ƒã«ãƒãƒƒãƒ”ãƒ³ã‚°
        embedding = np.zeros(self.embedding_dim)
        for token, count in token_counts.items():
            token_hash = hash(token) % self.embedding_dim
            embedding[token_hash] += count
        
        # æ­£è¦åŒ–
        total_tokens = len(tokens)
        if total_tokens > 0:
            embedding = embedding / total_tokens
        
        # L2æ­£è¦åŒ–
        norm = np.linalg.norm(embedding)
        if norm > 0:
            embedding = embedding / norm
        
        return embedding
    
    def _tfidf_embedding(self, text: str) -> np.ndarray:
        """
        TF-IDF ãƒ™ãƒ¼ã‚¹åŸ‹ã‚è¾¼ã¿ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        
        Args:
            text: å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«
        """
        tokens = self._tokenize(text)
        if not tokens:
            return np.zeros(self.embedding_dim)
        
        # TFè¨ˆç®—
        token_counts = Counter(tokens)
        total_tokens = len(tokens)
        
        embedding = np.zeros(self.embedding_dim)
        for token, count in token_counts.items():
            # TF
            tf = count / total_tokens
            
            # IDFï¼ˆä»®ã®å€¤ã€å®Ÿéš›ã®ã‚³ãƒ¼ãƒ‘ã‚¹çµ±è¨ˆãªã—ï¼‰
            idf = self.idf_scores.get(token, 1.0)
            
            # TF-IDF
            tfidf = tf * idf
            
            # ãƒãƒƒã‚·ãƒ¥ãƒ™ãƒ¼ã‚¹ã§æ¬¡å…ƒã«ãƒãƒƒãƒ”ãƒ³ã‚°
            token_hash = hash(token) % self.embedding_dim
            embedding[token_hash] += tfidf
        
        # L2æ­£è¦åŒ–
        norm = np.linalg.norm(embedding)
        if norm > 0:
            embedding = embedding / norm
        
        return embedding
    
    def encode(self, texts: Union[str, List[str]], **kwargs) -> np.ndarray:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã‚’åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›
        
        Args:
            texts: å˜ä¸€ãƒ†ã‚­ã‚¹ãƒˆã¾ãŸã¯ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆ
            **kwargs: è¿½åŠ å¼•æ•°ï¼ˆäº’æ›æ€§ã®ãŸã‚ï¼‰
            
        Returns:
            åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«é…åˆ—
        """
        if isinstance(texts, str):
            texts = [texts]
        
        embeddings = []
        for text in texts:
            if self.method == 'hash':
                embedding = self._hash_embedding(text)
            elif self.method == 'bow':
                embedding = self._bow_embedding(text)
            elif self.method == 'tfidf':
                embedding = self._tfidf_embedding(text)
            else:
                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ãƒãƒƒã‚·ãƒ¥ãƒ™ãƒ¼ã‚¹
                embedding = self._hash_embedding(text)
            
            embeddings.append(embedding)
        
        return np.array(embeddings)
    
    def is_available(self) -> bool:
        """åŸ‹ã‚è¾¼ã¿æ©Ÿèƒ½ãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯"""
        return True  # å¸¸ã«åˆ©ç”¨å¯èƒ½


def get_sentence_transformer(model_name: str = 'simple', 
                           cache_folder: Optional[str] = None,
                           local_files_only: bool = True):
    """
    SimpleEmbedder ã‚’ SentenceTransformeräº’æ›ã§å–å¾—
    
    Args:
        model_name: ãƒ¢ãƒ‡ãƒ«åï¼ˆç„¡è¦–ã•ã‚Œã‚‹ï¼‰
        cache_folder: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚©ãƒ«ãƒ€ï¼ˆç„¡è¦–ã•ã‚Œã‚‹ï¼‰
        local_files_only: ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ï¼ˆç„¡è¦–ã•ã‚Œã‚‹ï¼‰
        
    Returns:
        SimpleEmbedderã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
    """
    logger.info(f"ğŸš€ SimpleEmbedderåˆæœŸåŒ–é–‹å§‹ï¼ˆSentenceTransformeräº’æ›ãƒ¢ãƒ¼ãƒ‰ï¼‰")
    
    embedder = SimpleEmbedder({
        'embedding_dim': 384,
        'embedding_method': 'hash',
        'vocab_size': 10000
    })
    
    logger.info(f"âœ… SimpleEmbedderåˆæœŸåŒ–å®Œäº†ï¼ˆè»½é‡åŸ‹ã‚è¾¼ã¿ï¼‰")
    return embedder


class SharedEmbedder:
    """
    SimpleEmbedderç”¨ã®å…±æœ‰ãƒ©ãƒƒãƒ‘ãƒ¼
    """
    
    def __init__(self, config: Dict[str, Any]):
        """
        å…±æœ‰Embedderã®åˆæœŸåŒ–
        
        Args:
            config: è¨­å®šè¾æ›¸
        """
        self.config = config
        self.debug = config.get('debug', False)
        
        if self.debug:
            logger.setLevel(logging.DEBUG)
        
        # SimpleEmbedderã‚’ä½œæˆ
        self._embedder = SimpleEmbedder(config)
    
    @property
    def transformer(self):
        """
        SimpleEmbedderã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—
        
        Returns:
            SimpleEmbedderã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        """
        return self._embedder
    
    def encode(self, texts: Union[str, List[str]], **kwargs) -> Optional[np.ndarray]:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã‚’åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›
        
        Args:
            texts: å˜ä¸€ãƒ†ã‚­ã‚¹ãƒˆã¾ãŸã¯ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆ
            **kwargs: è¿½åŠ å¼•æ•°
            
        Returns:
            åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«é…åˆ— or None
        """
        try:
            if self.debug:
                logger.debug(f"ğŸ” åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ: {type(texts)} (len={len(texts) if isinstance(texts, list) else 1})")
            
            embeddings = self._embedder.encode(texts, **kwargs)
            
            if self.debug:
                logger.debug(f"âœ… åŸ‹ã‚è¾¼ã¿ç”Ÿæˆå®Œäº†: shape={embeddings.shape}")
            
            return embeddings
            
        except Exception as e:
            logger.error(f"âŒ åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def is_available(self) -> bool:
        """
        EmbedderãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
        
        Returns:
            åˆ©ç”¨å¯èƒ½æ€§
        """
        return True  # SimpleEmbedderã¯å¸¸ã«åˆ©ç”¨å¯èƒ½
