#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
MurmurNet å…±æœ‰Embedderãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
SentenceTransformerã®Singletonå®Ÿè£…
ãƒ—ãƒ­ã‚»ã‚¹èµ·å‹•æ™‚ã«ä¸€åº¦ã ã‘ãƒ­ãƒ¼ãƒ‰ã—ã€å…¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§å…±æœ‰

ä½œè€…: Yuhi Sonoki
"""

import logging
import os
import time
import threading
from functools import lru_cache
from typing import Dict, Any, Optional, List, Union
import numpy as np

logger = logging.getLogger('MurmurNet.Embedder')

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ï¼ˆãƒ—ãƒ­ã‚»ã‚¹å…¨ä½“ã§å…±æœ‰ï¼‰
_GLOBAL_EMBEDDER = None
_EMBEDDER_LOCK = threading.Lock()

@lru_cache(maxsize=1)
def get_sentence_transformer(model_name: str = 'all-MiniLM-L6-v2', 
                           cache_folder: Optional[str] = None,
                           local_files_only: bool = True):
    """
    SentenceTransformerã®Singletonå–å¾—
    
    @lru_cache ã«ã‚ˆã‚Šã€åŒã˜å¼•æ•°ã§ä½•åº¦å‘¼ã³å‡ºã•ã‚Œã¦ã‚‚
    åˆå›ã®ã¿ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã•ã‚Œã€ä»¥é™ã¯åŒã˜ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’è¿”ã™
    
    å¼•æ•°:
        model_name: ãƒ¢ãƒ‡ãƒ«å
        cache_folder: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚©ãƒ«ãƒ€
        local_files_only: ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ä½¿ç”¨
        
    æˆ»ã‚Šå€¤:
        SentenceTransformerã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
    """
    try:
        from sentence_transformers import SentenceTransformer
        
        logger.info(f"ğŸš€ SentenceTransformeråˆæœŸåŒ–é–‹å§‹: {model_name}")
        start_time = time.time()
        
        # ã‚ªãƒ•ãƒ©ã‚¤ãƒ³å®Ÿè¡Œã«æœ€é©åŒ–ã•ã‚ŒãŸã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š
        if cache_folder is None:
            cache_folder = os.path.join(os.path.expanduser("~"), ".cache", "sentence_transformers")
        
        os.makedirs(cache_folder, exist_ok=True)
        
        # ãƒ¢ãƒ‡ãƒ«åã®æ­£è¦åŒ–ï¼ˆsentence-transformers/ ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’é™¤å»ï¼‰
        if model_name.startswith('sentence-transformers/'):
            model_name = model_name[len('sentence-transformers/'):]
        
        transformer = SentenceTransformer(
            model_name,
            local_files_only=local_files_only,
            cache_folder=cache_folder,
            trust_remote_code=False  # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¼·åŒ–
        )
        
        load_time = time.time() - start_time
        logger.info(f"âœ… SentenceTransformeråˆæœŸåŒ–å®Œäº†: {model_name} ({load_time:.2f}s)")
        return transformer
        
    except ImportError:
        logger.error("âŒ SentenceTransformersãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        # SimpleEmbedderã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        from .simple_embedder import get_sentence_transformer as get_simple_embedder
        return get_simple_embedder()
    except Exception as e:
        logger.error(f"âŒ SentenceTransformeråˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        # SimpleEmbedderã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        from .simple_embedder import get_sentence_transformer as get_simple_embedder
        return get_simple_embedder()

class SharedEmbedder:
    """
    å…±æœ‰Embedderã‚¯ãƒ©ã‚¹
    
    å…¨ã¦ã®InputReceptionã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã§åŒã˜SentenceTransformerã‚’å…±æœ‰
    ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã¨ãƒ­ãƒ¼ãƒ‰æ™‚é–“ã‚’å¤§å¹…ã«å‰Šæ¸›
    """
    
    def __init__(self, config: Dict[str, Any]):
        """
        å…±æœ‰Embedderã®åˆæœŸåŒ–
        
        å¼•æ•°:
            config: è¨­å®šè¾æ›¸
        """
        self.config = config
        self.model_name = config.get('embedding_model', 'all-MiniLM-L6-v2')
        self.cache_folder = config.get('cache_folder', None)
        self.local_files_only = config.get('local_files_only', True)
        self.debug = config.get('debug', False)
        
        if self.debug:
            logger.setLevel(logging.DEBUG)
    
    @property
    def transformer(self):
        """
        SentenceTransformerã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—
        
        åˆå›ã‚¢ã‚¯ã‚»ã‚¹æ™‚ã®ã¿ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã€ä»¥é™ã¯åŒã˜ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’è¿”ã™
        
        æˆ»ã‚Šå€¤:
            SentenceTransformerã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ or None
        """
        return get_sentence_transformer(
            self.model_name,
            self.cache_folder,
            self.local_files_only
        )
    
    def encode(self, texts: Union[str, List[str]], **kwargs) -> Optional[np.ndarray]:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã‚’åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›
        
        å¼•æ•°:
            texts: å˜ä¸€ãƒ†ã‚­ã‚¹ãƒˆã¾ãŸã¯ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆ
            **kwargs: SentenceTransformer.encodeã®è¿½åŠ å¼•æ•°
            
        æˆ»ã‚Šå€¤:
            åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«é…åˆ— or None
        """
        transformer = self.transformer
        if transformer is None:
            logger.warning("âš ï¸ SentenceTransformerãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
            return None
        
        try:
            if self.debug:
                logger.debug(f"ğŸ” åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ: {type(texts)} (len={len(texts) if isinstance(texts, list) else 1})")
            
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
            encode_kwargs = {
                'show_progress_bar': False,
                'convert_to_numpy': True,
                'normalize_embeddings': True,
                **kwargs
            }
            
            embeddings = transformer.encode(texts, **encode_kwargs)
            
            if self.debug:
                shape = embeddings.shape if hasattr(embeddings, 'shape') else 'unknown'
                logger.debug(f"âœ… åŸ‹ã‚è¾¼ã¿ç”Ÿæˆå®Œäº†: shape={shape}")
            
            return embeddings
            
        except Exception as e:
            logger.error(f"âŒ åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def is_available(self) -> bool:
        """
        EmbedderãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
        
        æˆ»ã‚Šå€¤:
            True: åˆ©ç”¨å¯èƒ½, False: åˆ©ç”¨ä¸å¯
        """
        return self.transformer is not None
    
    def get_embedding_dim(self) -> int:
        """
        åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒæ•°ã‚’å–å¾—
        
        æˆ»ã‚Šå€¤:
            åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒæ•°
        """
        transformer = self.transformer
        if transformer is None:
            return 384  # all-MiniLM-L6-v2ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ¬¡å…ƒæ•°
        
        try:
            # ãƒ€ãƒŸãƒ¼ãƒ†ã‚­ã‚¹ãƒˆã§æ¬¡å…ƒæ•°ã‚’ç¢ºèª
            dummy_embedding = transformer.encode(["test"], show_progress_bar=False)
            return dummy_embedding.shape[1]
        except Exception:
            return 384

# ä¾¿åˆ©é–¢æ•°
def get_shared_embedder(config: Dict[str, Any]) -> SharedEmbedder:
    """
    å…±æœ‰Embedderã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—
    
    å¼•æ•°:
        config: è¨­å®šè¾æ›¸
        
    æˆ»ã‚Šå€¤:
        SharedEmbedderã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
    """
    return SharedEmbedder(config)

# å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹
def get_global_embedder(config: Dict[str, Any]) -> SharedEmbedder:
    """å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹"""
    return get_shared_embedder(config)

# Embedderã‚¯ãƒ©ã‚¹ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹ï¼ˆå¾Œæ–¹äº’æ›æ€§ï¼‰
class Embedder(SharedEmbedder):
    """
    Embedderã‚¯ãƒ©ã‚¹ï¼ˆSharedEmbedderã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹ï¼‰
    å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã«æä¾›
    """
    
    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)
        self._initialized = False
    
    def initialize(self) -> bool:
        """
        Embedderã®åˆæœŸåŒ–
        SharedEmbedderã¯è‡ªå‹•åˆæœŸåŒ–ã®ãŸã‚ã€å®Ÿéš›ã®å‡¦ç†ã¯ä¸è¦
        
        æˆ»ã‚Šå€¤:
            True: åˆæœŸåŒ–æˆåŠŸ, False: åˆæœŸåŒ–å¤±æ•—
        """
        try:
            # SentenceTransformerã®å¯ç”¨æ€§ã‚’ãƒã‚§ãƒƒã‚¯
            available = self.is_available()
            self._initialized = available
            
            if available and self.debug:
                logger.info("âœ… EmbedderåˆæœŸåŒ–å®Œäº†")
            elif not available:
                logger.warning("âš ï¸ EmbedderåˆæœŸåŒ–å¤±æ•— - SentenceTransformerãŒåˆ©ç”¨ä¸å¯")
            
            return available
            
        except Exception as e:
            logger.error(f"âŒ EmbedderåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            self._initialized = False
            return False
    
    def embed_text(self, text: str) -> Optional[np.ndarray]:
        """
        å˜ä¸€ãƒ†ã‚­ã‚¹ãƒˆã®åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
        
        å¼•æ•°:
            text: åŸ‹ã‚è¾¼ã¿å¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆ
            
        æˆ»ã‚Šå€¤:
            åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ« or None
        """
        return self.encode(text)
    
    def embed_texts(self, texts: List[str]) -> Optional[np.ndarray]:
        """
        è¤‡æ•°ãƒ†ã‚­ã‚¹ãƒˆã®åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
        
        å¼•æ•°:
            texts: åŸ‹ã‚è¾¼ã¿å¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆ
            
        æˆ»ã‚Šå€¤:
            åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«é…åˆ— or None
        """
        return self.encode(texts)
    
    @property
    def is_initialized(self) -> bool:
        """åˆæœŸåŒ–çŠ¶æ…‹ã‚’å–å¾—"""
        return getattr(self, '_initialized', False)

if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆç”¨ã‚³ãƒ¼ãƒ‰
    import time
    
    logging.basicConfig(level=logging.INFO)
    
    config = {
        'embedding_model': 'all-MiniLM-L6-v2',
        'local_files_only': True,
        'debug': True
    }
    
    print("=== SharedEmbedder ãƒ†ã‚¹ãƒˆ ===")
    
    # 1å›ç›®ã®ãƒ­ãƒ¼ãƒ‰
    embedder1 = get_shared_embedder(config)
    start_time = time.time()
    embedding1 = embedder1.encode("Hello, world!")
    time1 = time.time() - start_time
    print(f"1å›ç›®: {time1:.3f}s, shape: {embedding1.shape if embedding1 is not None else 'None'}")
    
    # 2å›ç›®ã®ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ï¼‰
    embedder2 = get_shared_embedder(config)
    start_time = time.time()
    embedding2 = embedder2.encode("Hello, world!")
    time2 = time.time() - start_time
    print(f"2å›ç›®: {time2:.3f}s, shape: {embedding2.shape if embedding2 is not None else 'None'}")
    
    # åŒã˜ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‹ãƒã‚§ãƒƒã‚¯
    print(f"åŒã˜transformerã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹: {embedder1.transformer is embedder2.transformer}")
    print(f"é€Ÿåº¦å‘ä¸Š: {time1/time2:.1f}å€")
