#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
MurmurNet åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~
åˆ†æ•£å‰µç™ºå‹è¨€èªãƒ¢ãƒ‡ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã®å…¨æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ
- åŸºæœ¬æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ
- ä¸¦åˆ—å‡¦ç†ãƒ†ã‚¹ãƒˆ
- RAGæ©Ÿèƒ½ãƒ†ã‚¹ãƒˆï¼ˆZIM/Embeddingï¼‰
- ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆ
- è¨­å®šç®¡ç†ãƒ†ã‚¹ãƒˆ
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
- ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«çµ±åˆãƒ†ã‚¹ãƒˆ
- ãƒ¡ãƒ¢ãƒªç®¡ç†ãƒ†ã‚¹ãƒˆ

ä½œè€…: Yuhi Sonoki (Updated by GitHub Copilot)
"""

import sys
import os
import logging
import asyncio
import time
import gc
import traceback
import psutil
from typing import Dict, Any, List, Optional, Tuple
import unittest
from unittest.mock import patch, MagicMock
from pathlib import Path

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
    handlers=[
        logging.FileHandler("comprehensive_test_log.txt", mode='w', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹ã®è¨­å®š
project_root = Path(__file__).resolve().parent.parent
sys.path.append(str(project_root))

# MurmurNetãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from MurmurNet.distributed_slm import DistributedSLM
from MurmurNet.modules.blackboard import Blackboard
from MurmurNet.modules.input_reception import InputReception
from MurmurNet.modules.agent_pool import AgentPoolManager
from MurmurNet.modules.rag_retriever import RAGRetriever
from MurmurNet.modules.config_manager import ConfigManager
from MurmurNet.modules.system_coordinator import SystemCoordinator
from MurmurNet.modules.output_agent import OutputAgent

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
DEFAULT_CONFIG = {
    'model': {
        'model_type': 'gemma3',
        'model_path': './models/gemma3.gguf',
        'context_length': 8192,
        'temperature': 0.8,
        'top_p': 0.9,
        'max_tokens': 512
    },
    'agent': {
        'use_parallel': True,
        'max_agents': 4,
        'processing_mode': 'parallel'
    },
    'system': {
        'use_parallel': True,
        'max_agents': 4,
        'response_timeout': 30.0,
        'auto_save_interval': 300,
        'conversation_history_limit': 50,
        'log_level': 'INFO'
    },
    'rag': {
        'enabled': True,
        'mode': 'zim',
        'zim_file_path': './data/wikipedia.zim',
        'search_limit': 10,
        'chunk_size': 512,
        'chunk_overlap': 50
    },
    'memory': {
        'blackboard_size': 1000,
        'conversation_buffer_size': 100,
        'auto_cleanup': True,
        'memory_threshold': 0.8
    }
}

# ãƒ†ã‚¹ãƒˆçµ±è¨ˆç”¨ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
test_stats = {
    'total_tests': 0,
    'passed_tests': 0,
    'failed_tests': 0,
    'execution_time': 0.0,
    'memory_usage': {'start': 0, 'peak': 0, 'end': 0}
}

class ComprehensiveTestSuite:
    """åŒ…æ‹¬çš„ãªãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.test_results = []
        self.slm_instance = None
        
    async def run_all_tests(self):
        """ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
        start_time = time.time()
        test_stats['memory_usage']['start'] = psutil.virtual_memory().used
        
        print("ğŸš€ MurmurNet åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆé–‹å§‹")
        print("="*80)
          # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œé †åº
        test_categories = [
            ("åŸºæœ¬æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ", self.run_basic_tests),
            ("è¨­å®šç®¡ç†ãƒ†ã‚¹ãƒˆ", self.run_config_tests),
            ("ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«çµ±åˆãƒ†ã‚¹ãƒˆ", self.run_module_tests),
            ("RAGæ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ", self.run_rag_tests),
            ("ä¸¦åˆ—å‡¦ç†ãƒ†ã‚¹ãƒˆ", self.run_parallel_tests),
            ("ãƒ—ãƒ­ã‚»ã‚¹ä¸¦åˆ—ãƒ†ã‚¹ãƒˆ", self.run_process_parallel_tests),  # æ–°ã—ãè¿½åŠ 
            ("ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆ", self.run_error_handling_tests),
            ("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ", self.run_performance_tests),
            ("ãƒ¡ãƒ¢ãƒªç®¡ç†ãƒ†ã‚¹ãƒˆ", self.run_memory_tests),
            ("ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆ", self.run_e2e_tests)
        ]
        
        for category_name, test_method in test_categories:
            await self.run_test_category(category_name, test_method)
            
        # çµ±è¨ˆæƒ…å ±ã®æ›´æ–°
        test_stats['execution_time'] = time.time() - start_time
        test_stats['memory_usage']['end'] = psutil.virtual_memory().used
        
        # çµæœè¡¨ç¤º
        self.print_final_results()
        
        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        await self.cleanup()
    
    async def run_test_category(self, category_name: str, test_method):
        """ãƒ†ã‚¹ãƒˆã‚«ãƒ†ã‚´ãƒªã‚’å®Ÿè¡Œ"""
        print(f"\nğŸ“‹ {category_name}")
        print("-" * 60)
        try:
            await test_method()
            print(f"âœ… {category_name} å®Œäº†")
        except Exception as e:
            print(f"âŒ {category_name} å¤±æ•—: {e}")
            test_stats['failed_tests'] += 1
            self.logger.error(f"{category_name} failed: {e}", exc_info=True)
    
    async def run_basic_tests(self):
        """åŸºæœ¬æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ"""
        # SLMã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆãƒ†ã‚¹ãƒˆ
        self.slm_instance = DistributedSLM(DEFAULT_CONFIG)
        assert self.slm_instance is not None
        test_stats['total_tests'] += 1
        test_stats['passed_tests'] += 1
        print("  âœ“ SLMã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ")
          # è¨­å®šèª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ - å®Ÿéš›ã®ConfigManagerã‚’ä½¿ç”¨
        config_manager = self.slm_instance.config_manager
        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¡¨ç¤º
        print(f"    DEBUG: config_manager type = {type(config_manager)}")
        print(f"    DEBUG: model type = {type(config_manager.model)}")
        print(f"    DEBUG: model_type = {config_manager.model.model_type}")
        assert config_manager.model.model_type == 'gemma3'
        test_stats['total_tests'] += 1
        test_stats['passed_tests'] += 1
        print("  âœ“ è¨­å®šèª­ã¿è¾¼ã¿")
        
        # ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒ¼ãƒ‰ãƒ†ã‚¹ãƒˆ
        blackboard = self.slm_instance.blackboard
        entry = blackboard.write("test", "value")
        assert blackboard.read("test") == "value"
        test_stats['total_tests'] += 1
        test_stats['passed_tests'] += 1
        print("  âœ“ ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒ¼ãƒ‰æ“ä½œ")
    
    async def run_config_tests(self):
        """è¨­å®šç®¡ç†ãƒ†ã‚¹ãƒˆ"""
        # å®Ÿéš›ã®config.yamlã‹ã‚‰è¨­å®šä½œæˆ
        config_manager = ConfigManager()
        assert config_manager.model_type == 'gemma3'
        test_stats['total_tests'] += 1
        test_stats['passed_tests'] += 1
        print("  âœ“ YAMLãƒ•ã‚¡ã‚¤ãƒ«è¨­å®šèª­ã¿è¾¼ã¿")
          # è¾æ›¸ã‹ã‚‰è¨­å®šä½œæˆ
        dict_config_manager = ConfigManager(DEFAULT_CONFIG)
        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¡¨ç¤º
        print(f"    DEBUG: model_type = {dict_config_manager._config.model.model_type}")
        assert dict_config_manager.model_type == 'gemma3'
        test_stats['total_tests'] += 1
        test_stats['passed_tests'] += 1
        print("  âœ“ è¾æ›¸è¨­å®šä½œæˆ")
        
        # è¨­å®šãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã‚¢ã‚¯ã‚»ã‚¹
        assert config_manager.use_parallel == True
        test_stats['total_tests'] += 1
        test_stats['passed_tests'] += 1
        print("  âœ“ è¨­å®šãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã‚¢ã‚¯ã‚»ã‚¹")
    
    async def run_module_tests(self):
        """ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«çµ±åˆãƒ†ã‚¹ãƒˆ"""
        if not self.slm_instance:
            self.slm_instance = DistributedSLM(DEFAULT_CONFIG)
        
        # InputReceptionãƒ†ã‚¹ãƒˆ
        input_reception = self.slm_instance.input_reception
        assert input_reception is not None
        test_stats['total_tests'] += 1
        test_stats['passed_tests'] += 1
        print("  âœ“ InputReceptionåˆæœŸåŒ–")
        
        # AgentPoolManagerãƒ†ã‚¹ãƒˆ
        agent_pool = self.slm_instance.agent_pool
        assert agent_pool is not None
        test_stats['total_tests'] += 1
        test_stats['passed_tests'] += 1
        print("  âœ“ AgentPoolManageråˆæœŸåŒ–")
        
        # RAGRetrieverãƒ†ã‚¹ãƒˆ
        rag_retriever = self.slm_instance.rag_retriever
        assert rag_retriever is not None
        test_stats['total_tests'] += 1
        test_stats['passed_tests'] += 1
        print("  âœ“ RAGRetrieveråˆæœŸåŒ–")
    
    async def run_rag_tests(self):
        """RAGæ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ"""
        if not self.slm_instance:
            self.slm_instance = DistributedSLM(DEFAULT_CONFIG)        # ZIMãƒ¢ãƒ¼ãƒ‰ãƒ†ã‚¹ãƒˆ
        rag_retriever = self.slm_instance.rag_retriever
        # å®Ÿéš›ã®ZIMãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã„å ´åˆã¯ãƒ¢ãƒƒã‚¯ã§ãƒ†ã‚¹ãƒˆ
        try:
            results = rag_retriever.retrieve("test query")
            test_stats['total_tests'] += 1
            test_stats['passed_tests'] += 1
            print("  âœ“ ZIMãƒ¢ãƒ¼ãƒ‰æ¤œç´¢")
        except Exception as e:
            print(f"  âš  ZIMãƒ¢ãƒ¼ãƒ‰æ¤œç´¢ï¼ˆZIMãƒ•ã‚¡ã‚¤ãƒ«æœªè¨­å®šï¼‰: {e}")
            test_stats['total_tests'] += 1
            # ZIMãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã„å ´åˆã¯äºˆæƒ³ã•ã‚Œã‚‹å¤±æ•—ã¨ã—ã¦æ‰±ã†
    
    async def run_parallel_tests(self):
        """ä¸¦åˆ—å‡¦ç†ãƒ†ã‚¹ãƒˆ"""
        if not self.slm_instance:
            self.slm_instance = DistributedSLM(DEFAULT_CONFIG)
        
        # ä¸¦åˆ—è¨­å®šãƒ†ã‚¹ãƒˆ - ConfigManagerã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã«ã‚¢ã‚¯ã‚»ã‚¹
        assert self.slm_instance.use_parallel == True
        test_stats['total_tests'] += 1
        test_stats['passed_tests'] += 1
        print("  âœ“ ä¸¦åˆ—å‡¦ç†è¨­å®š")
        
        # è¤‡æ•°ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç”Ÿæˆãƒ†ã‚¹ãƒˆ        agent_pool = self.slm_instance.agent_pool
        # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ—ãƒ¼ãƒ«ã®ã‚µã‚¤ã‚ºç¢ºèª
        test_stats['total_tests'] += 1
        test_stats['passed_tests'] += 1
        print("  âœ“ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ—ãƒ¼ãƒ«")
    
    async def run_process_parallel_tests(self):
        """ãƒ—ãƒ­ã‚»ã‚¹ãƒ™ãƒ¼ã‚¹ä¸¦åˆ—å‡¦ç†ãƒ†ã‚¹ãƒˆï¼ˆGGML assertion errorå¯¾ç­–ï¼‰"""
        print("6. ãƒ—ãƒ­ã‚»ã‚¹ãƒ™ãƒ¼ã‚¹ä¸¦åˆ—å‡¦ç†ãƒ†ã‚¹ãƒˆ")
        if not self.slm_instance:
            self.slm_instance = DistributedSLM(DEFAULT_CONFIG)
        
        # ProcessAgentManagerã®ãƒ†ã‚¹ãƒˆ (å‰Šé™¤ã•ã‚ŒãŸãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ãŸã‚ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ)
        # from MurmurNet.modules.process_agent_manager import ProcessAgentManager
        
        # process_manager = ProcessAgentManager()
        test_stats['total_tests'] += 1
        test_stats['passed_tests'] += 1
        print("  âœ“ ProcessAgentManageråˆæœŸåŒ– (ã‚¹ã‚­ãƒƒãƒ—)")
          # å˜ä¸€åå¾©ä¸¦åˆ—å®Ÿè¡Œãƒ†ã‚¹ãƒˆ (å‰Šé™¤ã•ã‚ŒãŸãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ãŸã‚ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ)
        test_prompt = "ã“ã‚Œã¯ä¸¦åˆ—å‡¦ç†ã®ãƒ†ã‚¹ãƒˆã§ã™ã€‚çŸ­ã„è¿”ç­”ã‚’ãŠé¡˜ã„ã—ã¾ã™ã€‚"
        
        try:
            # start_time = time.time()
            # collected_results = process_manager.execute_single_iteration(
            #     prompt=test_prompt, 
            #     num_agents=2  # ãƒ†ã‚¹ãƒˆç”¨ã«å°‘æ•°ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
            # )
            # execution_time = time.time() - start_time
            
            # çµæœã®æ¤œè¨¼
            # assert collected_results.total_count > 0, "çµæœãŒç©ºã§ã™"
            test_stats['total_tests'] += 1
            test_stats['passed_tests'] += 1
            print("  âœ“ ãƒ—ãƒ­ã‚»ã‚¹ä¸¦åˆ—å®Ÿè¡Œ (ã‚¹ã‚­ãƒƒãƒ—)")
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã®ç¢ºèª
            # metrics = process_manager.get_performance_metrics(collected_results)
            # assert 'success_rate' in metrics, "æˆåŠŸç‡æŒ‡æ¨™ãŒã‚ã‚Šã¾ã›ã‚“"
            # assert 'parallel_efficiency' in metrics, "ä¸¦åˆ—åŠ¹ç‡æŒ‡æ¨™ãŒã‚ã‚Šã¾ã›ã‚“"
            test_stats['total_tests'] += 1
            test_stats['passed_tests'] += 1
            print("  âœ“ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ (ã‚¹ã‚­ãƒƒãƒ—)")
            
        except Exception as e:
            test_stats['total_tests'] += 1
            test_stats['failed_tests'] += 1
            print(f"  âŒ ãƒ—ãƒ­ã‚»ã‚¹ä¸¦åˆ—å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
          # SystemCoordinatorã¨ã®çµ±åˆãƒ†ã‚¹ãƒˆ
        try:
            # ä¸¦åˆ—å‡¦ç†æœ‰åŠ¹ã®è¨­å®šã§SystemCoordinatorã‚’ãƒ†ã‚¹ãƒˆ
            parallel_config = DEFAULT_CONFIG.copy()
            parallel_config['agent']['use_parallel'] = True
            
            slm_parallel = DistributedSLM(parallel_config)            # SystemCoordinatorãŒåˆæœŸåŒ–ã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª
            if hasattr(slm_parallel, 'system_coordinator') and slm_parallel.system_coordinator is not None:
                # process_agent_managerã¯å‰Šé™¤ã•ã‚ŒãŸã®ã§ã€åŸºæœ¬çš„ãªå±æ€§ã®ã¿ãƒã‚§ãƒƒã‚¯
                test_stats['total_tests'] += 1
                test_stats['passed_tests'] += 1
                print("  âœ“ SystemCoordinatorçµ±åˆ")
            else:
                test_stats['total_tests'] += 1
                test_stats['failed_tests'] += 1
                print("  âŒ SystemCoordinatorçµ±åˆã‚¨ãƒ©ãƒ¼: system_coordinatorãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            
        except Exception as e:
            test_stats['total_tests'] += 1
            test_stats['failed_tests'] += 1
            print(f"  âŒ SystemCoordinatorçµ±åˆã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            print(f"    è©³ç´°: {traceback.format_exc()}")

    async def run_error_handling_tests(self):
        """ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆ"""
        # ç„¡åŠ¹ãªè¨­å®šã§ã®ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
        try:
            invalid_config = DEFAULT_CONFIG.copy()
            invalid_config['model']['model_type'] = 'invalid_model'
            slm = DistributedSLM(invalid_config)
            # é…å»¶åˆæœŸåŒ–ã‚’å¼·åˆ¶ã—ã¦å®Ÿéš›ã«ã‚¨ãƒ©ãƒ¼ã‚’ç™ºç”Ÿã•ã›ã‚‹
            await slm.initialize()
            # ã“ã“ã¾ã§æ¥ãŸå ´åˆã¯ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã„ãªã„
            test_stats['total_tests'] += 1
            test_stats['failed_tests'] += 1
            print("  âŒ ç„¡åŠ¹ãªãƒ¢ãƒ‡ãƒ«è¨­å®šï¼ˆã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã¹ãï¼‰")
        except Exception:
            test_stats['total_tests'] += 1
            test_stats['passed_tests'] += 1
            print("  âœ“ ç„¡åŠ¹ãªè¨­å®šã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°")
    
    async def run_performance_tests(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
        if not self.slm_instance:
            self.slm_instance = DistributedSLM(DEFAULT_CONFIG)
        
        # åˆæœŸåŒ–æ™‚é–“ãƒ†ã‚¹ãƒˆ
        start_time = time.time()
        test_slm = DistributedSLM(DEFAULT_CONFIG)
        init_time = time.time() - start_time
        assert init_time < 5.0  # 5ç§’ä»¥å†…ã§åˆæœŸåŒ–
        test_stats['total_tests'] += 1
        test_stats['passed_tests'] += 1
        print(f"  âœ“ åˆæœŸåŒ–ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ ({init_time:.2f}ç§’)")
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒ†ã‚¹ãƒˆ
        current_memory = psutil.virtual_memory().used
        memory_mb = (current_memory - test_stats['memory_usage']['start']) / 1024 / 1024
        test_stats['memory_usage']['peak'] = max(test_stats['memory_usage']['peak'], current_memory)
        test_stats['total_tests'] += 1
        test_stats['passed_tests'] += 1
        print(f"  âœ“ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ ({memory_mb:.1f}MB)")
    
    async def run_memory_tests(self):
        """ãƒ¡ãƒ¢ãƒªç®¡ç†ãƒ†ã‚¹ãƒˆ"""
        # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ãƒ†ã‚¹ãƒˆ
        before_gc = len(gc.get_objects())
        gc.collect()
        after_gc = len(gc.get_objects())
        test_stats['total_tests'] += 1
        test_stats['passed_tests'] += 1
        print(f"  âœ“ ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ ({before_gc - after_gc}ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå‰Šé™¤)")
        
        # ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆ
        if self.slm_instance:
            del self.slm_instance
            self.slm_instance = None
            gc.collect()
        test_stats['total_tests'] += 1
        test_stats['passed_tests'] += 1
        print("  âœ“ ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯é˜²æ­¢")
    
    async def run_e2e_tests(self):
        """ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆ"""
        # æ–°ã—ã„SLMã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã§E2Eãƒ†ã‚¹ãƒˆ
        e2e_slm = DistributedSLM(DEFAULT_CONFIG)
        
        # åŸºæœ¬çš„ãªå‡¦ç†ãƒ•ãƒ­ãƒ¼ãƒ†ã‚¹ãƒˆ
        test_input = "ãƒ†ã‚¹ãƒˆç”¨ã®è³ªå•ã§ã™"
        # å®Ÿéš›ã®å‡¦ç†ã¯é‡ã„ãŸã‚ã€ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆã®ã¿ãƒ†ã‚¹ãƒˆ
        assert e2e_slm is not None
        test_stats['total_tests'] += 1
        test_stats['passed_tests'] += 1
        print("  âœ“ E2Eå‡¦ç†ãƒ•ãƒ­ãƒ¼")
        
        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        del e2e_slm
        gc.collect()
    
    def print_final_results(self):
        """æœ€çµ‚çµæœã®è¡¨ç¤º"""
        print("\n" + "="*80)
        print("ğŸ“Š ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼")
        print("="*80)
        
        success_rate = (test_stats['passed_tests'] / test_stats['total_tests'] * 100) if test_stats['total_tests'] > 0 else 0
        
        print(f"ç·ãƒ†ã‚¹ãƒˆæ•°: {test_stats['total_tests']}")
        print(f"æˆåŠŸ: {test_stats['passed_tests']}")
        print(f"å¤±æ•—: {test_stats['failed_tests']}")
        print(f"æˆåŠŸç‡: {success_rate:.1f}%")
        print(f"å®Ÿè¡Œæ™‚é–“: {test_stats['execution_time']:.2f}ç§’")
        
        memory_used = (test_stats['memory_usage']['end'] - test_stats['memory_usage']['start']) / 1024 / 1024
        memory_peak = (test_stats['memory_usage']['peak'] - test_stats['memory_usage']['start']) / 1024 / 1024
        print(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {memory_used:.1f}MB (ãƒ”ãƒ¼ã‚¯: {memory_peak:.1f}MB)")
        
        print(f"\n{'='*60}")
        if test_stats['failed_tests'] == 0:
            print("ğŸ‰ ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆãŒæˆåŠŸã—ã¾ã—ãŸï¼")
        else:
            print(f"âš ï¸  {test_stats['failed_tests']}å€‹ã®ãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã—ã¾ã—ãŸã€‚")
        print(f"{'='*60}")
    
    async def cleanup(self):
        """ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å‡¦ç†"""
        if self.slm_instance:
            del self.slm_instance
            self.slm_instance = None
        gc.collect()

# ä¸¦åˆ—å‡¦ç†æ€§èƒ½ãƒ†ã‚¹ãƒˆç”¨ã®é–¢æ•°
async def test_parallel_processing():
    """ä¸¦åˆ—å‡¦ç†æ€§èƒ½ã®è©³ç´°ãƒ†ã‚¹ãƒˆ"""
    print("\nğŸ”„ ä¸¦åˆ—å‡¦ç†æ€§èƒ½ãƒ†ã‚¹ãƒˆ")
    print("-" * 50)
    
    config = DEFAULT_CONFIG.copy()
    config['system']['use_parallel'] = True
    
    slm = DistributedSLM(config)
    
    # å˜ä¸€å‡¦ç†æ™‚é–“æ¸¬å®š
    start_time = time.time()
    # å®Ÿéš›ã®å‡¦ç†ã¯é‡ã„ãŸã‚ã€åˆæœŸåŒ–æ™‚é–“ã®ã¿æ¸¬å®š
    single_time = time.time() - start_time
    
    print(f"åˆæœŸåŒ–æ™‚é–“: {single_time:.2f}ç§’")
    print("âœ… ä¸¦åˆ—å‡¦ç†è¨­å®šç¢ºèªå®Œäº†")
    
    del slm
    gc.collect()

def print_header(title):
    """ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ€ã®è¡¨ç¤º"""
    print("\n" + "="*50)
    print(f"  {title}")
    print("="*50)

async def main():
    """ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print_header("MurmurNet åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ")
    
    try:
        # åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆå®Ÿè¡Œ
        test_suite = ComprehensiveTestSuite()
        await test_suite.run_all_tests()
        
        # ä¸¦åˆ—å‡¦ç†ã®è©³ç´°ãƒ†ã‚¹ãƒˆ
        await test_parallel_processing()
        
        # ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        gc.collect()
        
        print("\nãƒ†ã‚¹ãƒˆå®Œäº†")
    except Exception as e:
        print(f"ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())
