num_agents: 2
iterations: 1
use_summary: true
use_parallel: false
use_memory: true
performance_monitoring: true
memory_tracking: true
debug: false

# ログ設定
log_level: INFO
log_file: logs/murmurnet.log

# モデル設定
model_type: llama
model_path: "C:\\Users\\admin\\Desktop\\課題研究\\models\\gemma-3-1b-it-q4_0.gguf"
chat_template: "C:\\Users\\admin\\Desktop\\課題研究\\models\\gemma3_template.txt"
n_ctx: 1024  # 推論速度最適化（2048->1024でさらなる高速化）
n_threads: 6  # Ryzen 5 5600G最適化（4->6で推論速度向上）
n_batch: 64   # バッチサイズさらに拡大（5600G+RAM対応、15%高速化）
temperature: 0.7
max_tokens: 256
model_cache_dir: "cache/models"  # モデルキャッシュディレクトリ

# パフォーマンス設定
embedding_cache_size: 1000  # 埋め込みキャッシュサイズ
use_mmap: true              # メモリマップドファイル使用
use_mlock: false            # メモリロック無効（通常使用）

# エージェント設定
role_type: default

# RAG設定
rag_enabled: true
rag_mode: zim
zim_path: "C:\\Users\\admin\\Desktop\\課題研究\\KNOWAGE_DATABASE\\wikipedia_en_top_nopic_2025-03.zim"
rag_score_threshold: 0.5
rag_top_k: 3
embedding_model: "all-MiniLM-L6-v2"
model_cache_dir: "cache/sentence_transformers"  # SentenceTransformerキャッシュディレクトリ
offline_mode: true  # オフライン最優先モード（ネットワークアクセス回避）
embedding_force_offline: true  # 埋め込みモデルのオフライン強制
embedding_local_only: true  # ローカルキャッシュのみ使用

# 要約設定
summary_max_length: 200

# 会話記憶設定
conversation_memory_limit: 10
