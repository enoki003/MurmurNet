#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Phase 5-8 åˆ†æ•£ã‚·ã‚¹ãƒ†ãƒ æ‹¡å¼µæ©Ÿèƒ½ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
åˆ†æ•£å”èª¿ã€ç›£è¦–ã€ã‚ªãƒ¼ãƒˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–æ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆ

ä½œè€…: Yuhi Sonoki
"""

import asyncio
import logging
import sys
import os
import time
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from murmurnet.modules.distributed_coordination import create_distributed_coordinator, create_load_balancer
from murmurnet.modules.monitoring import create_metrics_collector, create_alert_manager
from murmurnet.modules.autoscaling import create_autoscaler, ScalingMetrics
from murmurnet.modules.performance_optimization import create_distributed_optimizer

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('test_distributed_extensions.log')
    ]
)

logger = logging.getLogger(__name__)

async def test_distributed_coordination():
    """åˆ†æ•£å”èª¿ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆ"""
    logger.info("=== Phase 5: åˆ†æ•£å”èª¿ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆé–‹å§‹ ===")
    
    config = {
        'heartbeat_interval': 2.0,
        'failure_timeout': 6.0,
        'consensus_algorithm': 'simple_majority',
        'load_balance_strategy': 'hybrid'
    }
    
    try:
        # åˆ†æ•£å”èª¿ã‚·ã‚¹ãƒ†ãƒ ä½œæˆãƒ»é–‹å§‹
        coordinator = create_distributed_coordinator(config)
        load_balancer = create_load_balancer(coordinator)
        
        await coordinator.start()
        await load_balancer.start()
        
        # ãƒ†ã‚¹ãƒˆã‚¿ã‚¹ã‚¯ã‚’æŠ•å…¥
        task_id = await coordinator.submit_task({
            'type': 'test_task',
            'data': 'Hello from distributed system!'
        })
        
        logger.info(f"ã‚¿ã‚¹ã‚¯æŠ•å…¥: {task_id}")
        
        # å°‘ã—å¾…ã£ã¦ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç¢ºèª
        await asyncio.sleep(3)
        
        status = coordinator.get_cluster_status()
        logger.info(f"ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼çŠ¶æ…‹: {status}")
        
        # åˆæ„ãƒ†ã‚¹ãƒˆ
        proposal = {'action': 'test_consensus', 'value': 42}
        consensus_result = await coordinator.achieve_consensus(proposal)
        logger.info(f"åˆæ„çµæœ: {consensus_result}")
        
        # åœæ­¢
        await coordinator.stop()
        await load_balancer.stop()
        
        logger.info("åˆ†æ•£å”èª¿ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆå®Œäº† âœ…")
        return True
        
    except Exception as e:
        logger.error(f"åˆ†æ•£å”èª¿ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return False

async def test_monitoring_system():
    """ç›£è¦–ãƒ»ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆ"""
    logger.info("=== Phase 6: ç›£è¦–ãƒ»ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆé–‹å§‹ ===")
    
    config = {
        'enable_prometheus': False,  # ãƒ†ã‚¹ãƒˆç’°å¢ƒã§ã¯Prometheusã‚µãƒ¼ãƒãƒ¼ã‚’ç„¡åŠ¹åŒ–
        'metrics_collection_interval': 1.0,
        'alert_check_interval': 2.0,
        'notification_channels': []
    }
    
    try:
        # ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ä½œæˆãƒ»é–‹å§‹
        metrics_collector = create_metrics_collector(config)
        alert_manager = create_alert_manager(config, metrics_collector)
        
        await metrics_collector.start()
        await alert_manager.start()
        
        # ãƒ†ã‚¹ãƒˆãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
        metrics_collector.record_metric('test_cpu_usage', 75.5)
        metrics_collector.record_metric('test_memory_usage', 60.2)
        metrics_collector.record_metric('test_requests_count', 100)
        
        # å°‘ã—å¾…ã£ã¦ãƒ¬ãƒãƒ¼ãƒˆå–å¾—
        await asyncio.sleep(2)
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹å±¥æ­´å–å¾—
        cpu_history = metrics_collector.get_metric_history('test_cpu_usage', 1)
        logger.info(f"CPUä½¿ç”¨ç‡å±¥æ­´: {len(cpu_history)} ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆ")
        
        # é›†ç´„ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—
        aggregated = metrics_collector.get_aggregated_metrics(1)
        logger.info(f"é›†ç´„ãƒ¡ãƒˆãƒªã‚¯ã‚¹: {aggregated}")
        
        # ã‚¢ãƒ©ãƒ¼ãƒˆçŠ¶æ…‹ç¢ºèª
        alert_status = alert_manager.get_alert_status()
        logger.info(f"ã‚¢ãƒ©ãƒ¼ãƒˆçŠ¶æ…‹: {alert_status}")
        
        # åœæ­¢
        await metrics_collector.stop()
        await alert_manager.stop()
        
        logger.info("ç›£è¦–ãƒ»ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆå®Œäº† âœ…")
        return True
        
    except Exception as e:
        logger.error(f"ç›£è¦–ãƒ»ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return False

async def test_autoscaling_system():
    """ã‚ªãƒ¼ãƒˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆ"""
    logger.info("=== Phase 7: ã‚ªãƒ¼ãƒˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆé–‹å§‹ ===")
    
    config = {
        'scaling_strategy': 'hybrid',
        'min_workers': 1,
        'max_workers': 3,
        'target_cpu_utilization': 0.7,
        'scale_up_threshold': 0.8,
        'scale_down_threshold': 0.3,
        'evaluation_period': 2,
        'use_kubernetes': False  # ãƒ†ã‚¹ãƒˆç’°å¢ƒã§ã¯Kubernetesç„¡åŠ¹åŒ–
    }
    
    try:
        # ã‚ªãƒ¼ãƒˆã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ä½œæˆãƒ»é–‹å§‹
        autoscaler = create_autoscaler(config)
        await autoscaler.start()
        
        # ãƒ†ã‚¹ãƒˆãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°ï¼ˆé«˜è² è·çŠ¶æ…‹ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼‰
        high_load_metrics = ScalingMetrics(
            cpu_usage=85.0,
            memory_usage=70.0,
            task_queue_length=60,
            active_workers=1,
            throughput=50.0
        )
        
        await autoscaler.update_metrics(high_load_metrics)
        logger.info("é«˜è² è·ãƒ¡ãƒˆãƒªã‚¯ã‚¹é€ä¿¡")
        
        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°åˆ¤å®šã‚’å¾…ã¤
        await asyncio.sleep(3)
        
        # çŠ¶æ…‹ç¢ºèª
        status = autoscaler.get_scaling_status()
        logger.info(f"ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°çŠ¶æ…‹: {status}")
        
        # ä½è² è·çŠ¶æ…‹ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        low_load_metrics = ScalingMetrics(
            cpu_usage=25.0,
            memory_usage=30.0,
            task_queue_length=5,
            active_workers=3,
            throughput=10.0
        )
        
        await autoscaler.update_metrics(low_load_metrics)
        logger.info("ä½è² è·ãƒ¡ãƒˆãƒªã‚¯ã‚¹é€ä¿¡")
        
        # å†åº¦ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°åˆ¤å®šã‚’å¾…ã¤
        await asyncio.sleep(3)
        
        final_status = autoscaler.get_scaling_status()
        logger.info(f"æœ€çµ‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°çŠ¶æ…‹: {final_status}")
        
        # åœæ­¢
        await autoscaler.stop()
        
        logger.info("ã‚ªãƒ¼ãƒˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆå®Œäº† âœ…")
        return True
        
    except Exception as e:
        logger.error(f"ã‚ªãƒ¼ãƒˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return False

async def test_performance_optimization():
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆ"""
    logger.info("=== Phase 8: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆé–‹å§‹ ===")
    
    config = {
        'enable_latency_tracing': True,
        'trace_sample_rate': 1.0,  # ãƒ†ã‚¹ãƒˆã§ã¯100%ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        'enable_memory_tracking': True,
        'enable_compression': True,
        'compression_algorithm': 'lz4',
        'enable_batching': True,
        'batch_size': 5,
        'enable_profiling': False,  # ãƒ†ã‚¹ãƒˆç’°å¢ƒã§ã¯ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ç„¡åŠ¹åŒ–
        'enable_auto_optimization': False  # æ‰‹å‹•ãƒ†ã‚¹ãƒˆã®ãŸã‚ç„¡åŠ¹åŒ–
    }
    
    try:
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ ä½œæˆãƒ»é–‹å§‹
        optimizer = create_distributed_optimizer(config)
        await optimizer.start()
        
        # ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·æ¸¬å®šãƒ†ã‚¹ãƒˆ
        @optimizer.latency_optimizer.measure_latency("test_function")
        async def test_async_function():
            await asyncio.sleep(0.1)  # 100ms ã®å‡¦ç†ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            return "test_result"
        
        # ãƒ†ã‚¹ãƒˆé–¢æ•°ã‚’æ•°å›å®Ÿè¡Œ
        for i in range(5):
            result = await test_async_function()
            logger.debug(f"ãƒ†ã‚¹ãƒˆé–¢æ•°å®Ÿè¡Œ {i+1}: {result}")
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆå–å¾—
        latency_report = optimizer.latency_optimizer.get_performance_report()
        logger.info(f"ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¬ãƒãƒ¼ãƒˆ: {latency_report}")
        
        # ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ãƒ†ã‚¹ãƒˆ
        optimizer.memory_optimizer.track_memory_usage()
        memory_report = optimizer.memory_optimizer.get_memory_report()
        logger.info(f"ãƒ¡ãƒ¢ãƒªãƒ¬ãƒãƒ¼ãƒˆ: {memory_report}")
        
        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æœ€é©åŒ–ãƒ†ã‚¹ãƒˆ
        test_data = {'message': 'Hello, world!', 'numbers': list(range(100))}
        
        # ãƒ‡ãƒ¼ã‚¿åœ§ç¸®ãƒ†ã‚¹ãƒˆ
        compressed_data = optimizer.network_optimizer.compress_data(test_data)
        decompressed_data = optimizer.network_optimizer.decompress_data(compressed_data)
        
        logger.info(f"åœ§ç¸®ãƒ†ã‚¹ãƒˆ: å…ƒãƒ‡ãƒ¼ã‚¿ == å¾©å…ƒãƒ‡ãƒ¼ã‚¿: {test_data == decompressed_data}")
        
        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯çµ±è¨ˆå–å¾—
        network_stats = optimizer.network_optimizer.get_network_stats()
        logger.info(f"ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯çµ±è¨ˆ: {network_stats}")
        
        # åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆå–å¾—
        comprehensive_report = optimizer.get_comprehensive_report()
        logger.info(f"åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆå–å¾—å®Œäº†: {len(comprehensive_report)} ã‚»ã‚¯ã‚·ãƒ§ãƒ³")
        
        # åœæ­¢
        await optimizer.stop()
        
        logger.info("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆå®Œäº† âœ…")
        return True
        
    except Exception as e:
        logger.error(f"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return False

async def test_integrated_system():
    """çµ±åˆã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ"""
    logger.info("=== çµ±åˆã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆé–‹å§‹ ===")
    
    # çµ±åˆè¨­å®š
    config = {
        # åˆ†æ•£å”èª¿
        'heartbeat_interval': 3.0,
        'consensus_algorithm': 'simple_majority',
        'load_balance_strategy': 'hybrid',
        
        # ç›£è¦–
        'enable_prometheus': False,
        'metrics_collection_interval': 2.0,
        'alert_check_interval': 5.0,
        
        # ã‚ªãƒ¼ãƒˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        'scaling_strategy': 'hybrid',
        'min_workers': 1,
        'max_workers': 2,
        'evaluation_period': 3,
        'use_kubernetes': False,
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–
        'enable_latency_tracing': True,
        'enable_memory_tracking': True,
        'enable_auto_optimization': True,
        'optimization_interval': 10
    }
    
    try:
        # å…¨ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        coordinator = create_distributed_coordinator(config)
        metrics_collector = create_metrics_collector(config)
        alert_manager = create_alert_manager(config, metrics_collector)
        autoscaler = create_autoscaler(config)
        optimizer = create_distributed_optimizer(config)
        
        # å…¨ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹
        await coordinator.start()
        await metrics_collector.start()
        await alert_manager.start()
        await autoscaler.start()
        await optimizer.start()
        
        logger.info("å…¨åˆ†æ•£ã‚·ã‚¹ãƒ†ãƒ ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆé–‹å§‹å®Œäº†")
        
        # ã‚·ã‚¹ãƒ†ãƒ é–“é€£æºãƒ†ã‚¹ãƒˆ
        for i in range(3):
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç”Ÿæˆ
            metrics_collector.record_metric('integrated_test_cpu', 50.0 + i * 10)
            metrics_collector.record_metric('integrated_test_memory', 40.0 + i * 15)
            
            # ã‚ªãƒ¼ãƒˆã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼ã«ãƒ¡ãƒˆãƒªã‚¯ã‚¹é€ä¿¡
            scaling_metrics = ScalingMetrics(
                cpu_usage=50.0 + i * 10,
                memory_usage=40.0 + i * 15,
                task_queue_length=10 + i * 5,
                active_workers=1,
                throughput=20.0 + i * 5
            )
            await autoscaler.update_metrics(scaling_metrics)
            
            # å”èª¿ã‚·ã‚¹ãƒ†ãƒ ã«ã‚¿ã‚¹ã‚¯æŠ•å…¥
            task_id = await coordinator.submit_task({
                'type': 'integrated_test',
                'iteration': i,
                'timestamp': time.time()
            })
            
            logger.info(f"çµ±åˆãƒ†ã‚¹ãƒˆåå¾© {i+1} å®Œäº† - ã‚¿ã‚¹ã‚¯ID: {task_id}")
            await asyncio.sleep(2)
        
        # æœ€çµ‚çŠ¶æ…‹ç¢ºèª
        cluster_status = coordinator.get_cluster_status()
        scaling_status = autoscaler.get_scaling_status()
        alert_status = alert_manager.get_alert_status()
        performance_report = optimizer.get_comprehensive_report()
        
        logger.info(f"æœ€çµ‚ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼çŠ¶æ…‹: ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒ¼ãƒ‰æ•°={cluster_status['active_nodes']}")
        logger.info(f"æœ€çµ‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°çŠ¶æ…‹: ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°={scaling_status['current_workers']}")
        logger.info(f"æœ€çµ‚ã‚¢ãƒ©ãƒ¼ãƒˆçŠ¶æ…‹: ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚¢ãƒ©ãƒ¼ãƒˆæ•°={alert_status['active_alerts']}")
        logger.info(f"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆå–å¾—: {len(performance_report)} ã‚»ã‚¯ã‚·ãƒ§ãƒ³")
        
        # å…¨ã‚·ã‚¹ãƒ†ãƒ åœæ­¢
        await optimizer.stop()
        await autoscaler.stop()
        await alert_manager.stop()
        await metrics_collector.stop()
        await coordinator.stop()
        
        logger.info("çµ±åˆã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆå®Œäº† âœ…")
        return True
        
    except Exception as e:
        logger.error(f"çµ±åˆã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return False

async def main():
    """ãƒ¡ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆé–¢æ•°"""
    logger.info("ğŸš€ Phase 5-8 åˆ†æ•£ã‚·ã‚¹ãƒ†ãƒ æ‹¡å¼µæ©Ÿèƒ½ãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    test_results = {}
    
    # å„ãƒ•ã‚§ãƒ¼ã‚ºã®ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    test_results['distributed_coordination'] = await test_distributed_coordination()
    test_results['monitoring_system'] = await test_monitoring_system()
    test_results['autoscaling_system'] = await test_autoscaling_system()
    test_results['performance_optimization'] = await test_performance_optimization()
    test_results['integrated_system'] = await test_integrated_system()
    
    # çµæœã‚µãƒãƒªãƒ¼
    logger.info("\n" + "="*60)
    logger.info("ğŸ“Š ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼")
    logger.info("="*60)
    
    passed_tests = 0
    total_tests = len(test_results)
    
    for test_name, result in test_results.items():
        status = "âœ… PASS" if result else "âŒ FAIL"
        logger.info(f"{test_name}: {status}")
        if result:
            passed_tests += 1
    
    logger.info("="*60)
    logger.info(f"ç·åˆçµæœ: {passed_tests}/{total_tests} ãƒ†ã‚¹ãƒˆé€šé")
    
    if passed_tests == total_tests:
        logger.info("ğŸ‰ å…¨ãƒ†ã‚¹ãƒˆæˆåŠŸï¼åˆ†æ•£ã‚·ã‚¹ãƒ†ãƒ æ‹¡å¼µæ©Ÿèƒ½ã¯æ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã¾ã™ã€‚")
        return 0
    else:
        logger.error("âš ï¸  ä¸€éƒ¨ãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return 1

if __name__ == "__main__":
    try:
        exit_code = asyncio.run(main())
        sys.exit(exit_code)
    except KeyboardInterrupt:
        logger.info("ãƒ†ã‚¹ãƒˆãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        sys.exit(1)
    except Exception as e:
        logger.error(f"ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(1)
